{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf2303e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (fallback if not already installed)\n",
    "import sys\n",
    "!{sys.executable} -m pip install pandas numpy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689e9d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Logistic Regression ---\n",
      "Logistic Regression trained successfully.\n",
      "\n",
      "--- Training Random Forest ---\n",
      "Random Forest trained successfully.\n",
      "\n",
      "--- Training Support Vector Machine ---\n",
      "Support Vector Machine trained successfully.\n",
      "\n",
      "--- Evaluating Logistic Regression ---\n",
      "Accuracy: 0.9710\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Low       0.99      1.00      0.99       250\n",
      "      Medium       0.97      0.98      0.97       250\n",
      "        High       0.97      0.96      0.97       250\n",
      "    Critical       0.96      0.94      0.95       250\n",
      "\n",
      "    accuracy                           0.97      1000\n",
      "   macro avg       0.97      0.97      0.97      1000\n",
      "weighted avg       0.97      0.97      0.97      1000\n",
      "\n",
      "--- Evaluating Random Forest ---\n",
      "Accuracy: 0.9420\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Low       0.98      0.98      0.98       250\n",
      "      Medium       0.92      0.94      0.93       250\n",
      "        High       0.95      0.96      0.95       250\n",
      "    Critical       0.92      0.88      0.90       250\n",
      "\n",
      "    accuracy                           0.94      1000\n",
      "   macro avg       0.94      0.94      0.94      1000\n",
      "weighted avg       0.94      0.94      0.94      1000\n",
      "\n",
      "--- Evaluating Support Vector Machine ---\n",
      "Accuracy: 0.9820\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Low       0.99      1.00      0.99       250\n",
      "      Medium       0.99      0.98      0.99       250\n",
      "        High       0.98      0.97      0.98       250\n",
      "    Critical       0.96      0.98      0.97       250\n",
      "\n",
      "    accuracy                           0.98      1000\n",
      "   macro avg       0.98      0.98      0.98      1000\n",
      "weighted avg       0.98      0.98      0.98      1000\n",
      "\n",
      "\n",
      "Best performing model: Support Vector Machine with an accuracy of 0.9820\n",
      "Best model (Support Vector Machine) saved to ../models\\best_model.pkl\n"
     ]
    }
   ],
   "source": [
    "# rockfall-prediction-system/notebooks/03_model_development.py\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Load the processed data\n",
    "processed_data_dir = '../data/processed'\n",
    "X_train = pd.read_csv(os.path.join(processed_data_dir, 'X_train.csv'))\n",
    "X_test = pd.read_csv(os.path.join(processed_data_dir, 'X_test.csv'))\n",
    "y_train = pd.read_csv(os.path.join(processed_data_dir, 'y_train.csv')).values.ravel()\n",
    "y_test = pd.read_csv(os.path.join(processed_data_dir, 'y_test.csv')).values.ravel()\n",
    "\n",
    "print(\"Data loaded successfully!\")\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561173a9",
   "metadata": {},
   "source": [
    "# Phase 1: Train Multiple Classification Models\n",
    "\n",
    "We'll evaluate 6 different classification algorithms to find the best baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6245a33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models with default parameters\n",
    "models = {\n",
    "    'Logistic Regression': Pipeline([\n",
    "        ('scaler', StandardScaler()), \n",
    "        ('clf', LogisticRegression(random_state=42, max_iter=1000))\n",
    "    ]),\n",
    "    'Random Forest': Pipeline([\n",
    "        ('scaler', StandardScaler()), \n",
    "        ('clf', RandomForestClassifier(random_state=42))\n",
    "    ]),\n",
    "    'Support Vector Machine': Pipeline([\n",
    "        ('scaler', StandardScaler()), \n",
    "        ('clf', SVC(random_state=42))\n",
    "    ]),\n",
    "    'Decision Tree': Pipeline([\n",
    "        ('scaler', StandardScaler()), \n",
    "        ('clf', DecisionTreeClassifier(random_state=42))\n",
    "    ]),\n",
    "    'Naive Bayes': Pipeline([\n",
    "        ('scaler', StandardScaler()), \n",
    "        ('clf', GaussianNB())\n",
    "    ]),\n",
    "    'K-Nearest Neighbors': Pipeline([\n",
    "        ('scaler', StandardScaler()), \n",
    "        ('clf', KNeighborsClassifier())\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Train and evaluate all models\n",
    "results = {}\n",
    "trained_models = {}\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING AND EVALUATING MODELS WITH DEFAULT PARAMETERS\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "for name, pipeline in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    \n",
    "    # Train the model\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    trained_models[name] = pipeline\n",
    "    \n",
    "    # Cross-validation score (5-fold)\n",
    "    cv_scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    cv_mean = cv_scores.mean()\n",
    "    cv_std = cv_scores.std()\n",
    "    \n",
    "    # Test set evaluation\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        'CV Mean': cv_mean,\n",
    "        'CV Std': cv_std,\n",
    "        'Test Accuracy': test_accuracy\n",
    "    }\n",
    "    \n",
    "    print(f\"  ✓ Cross-Validation Accuracy: {cv_mean:.4f} (+/- {cv_std:.4f})\")\n",
    "    print(f\"  ✓ Test Accuracy: {test_accuracy:.4f}\\n\")\n",
    "\n",
    "# Display results summary\n",
    "results_df = pd.DataFrame(results).T.sort_values('Test Accuracy', ascending=False)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(results_df)\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44575ef9",
   "metadata": {},
   "source": [
    "# Phase 2: Hyperparameter Tuning for Top Models\n",
    "\n",
    "Now we'll use GridSearchCV to find optimal hyperparameters for the best performing models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cecf411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top 3 models for hyperparameter tuning\n",
    "top_models = results_df.head(3).index.tolist()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"HYPERPARAMETER TUNING WITH GRIDSEARCHCV\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nTuning top {len(top_models)} models: {', '.join(top_models)}\\n\")\n",
    "\n",
    "# Define parameter grids for each model type\n",
    "param_grids = {\n",
    "    'Random Forest': {\n",
    "        'clf__n_estimators': [100, 200, 300],\n",
    "        'clf__max_depth': [10, 20, 30, None],\n",
    "        'clf__min_samples_split': [2, 5, 10],\n",
    "        'clf__min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'Support Vector Machine': {\n",
    "        'clf__C': [0.1, 1, 10, 100],\n",
    "        'clf__gamma': ['scale', 'auto', 0.001, 0.01],\n",
    "        'clf__kernel': ['rbf', 'linear']\n",
    "    },\n",
    "    'Logistic Regression': {\n",
    "        'clf__C': [0.01, 0.1, 1, 10, 100],\n",
    "        'clf__penalty': ['l2'],\n",
    "        'clf__solver': ['lbfgs', 'saga']\n",
    "    },\n",
    "    'K-Nearest Neighbors': {\n",
    "        'clf__n_neighbors': [3, 5, 7, 9, 11],\n",
    "        'clf__weights': ['uniform', 'distance'],\n",
    "        'clf__metric': ['euclidean', 'manhattan']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Perform grid search for top models\n",
    "tuned_models = {}\n",
    "tuning_results = {}\n",
    "\n",
    "for model_name in top_models:\n",
    "    if model_name in param_grids:\n",
    "        print(f\"Tuning {model_name}...\")\n",
    "        print(f\"  Parameter grid size: {np.prod([len(v) for v in param_grids[model_name].values()])} combinations\")\n",
    "        \n",
    "        # Create a fresh pipeline\n",
    "        base_pipeline = models[model_name]\n",
    "        \n",
    "        # Grid search with 3-fold CV (faster than 5-fold)\n",
    "        grid_search = GridSearchCV(\n",
    "            base_pipeline,\n",
    "            param_grids[model_name],\n",
    "            cv=3,\n",
    "            scoring='accuracy',\n",
    "            n_jobs=-1,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        # Store tuned model\n",
    "        tuned_models[model_name] = grid_search.best_estimator_\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        y_pred_tuned = grid_search.predict(X_test)\n",
    "        test_acc_tuned = accuracy_score(y_test, y_pred_tuned)\n",
    "        \n",
    "        tuning_results[model_name] = {\n",
    "            'Best CV Score': grid_search.best_score_,\n",
    "            'Test Accuracy': test_acc_tuned,\n",
    "            'Best Parameters': grid_search.best_params_,\n",
    "            'Improvement': test_acc_tuned - results[model_name]['Test Accuracy']\n",
    "        }\n",
    "        \n",
    "        print(f\"  ✓ Best CV Score: {grid_search.best_score_:.4f}\")\n",
    "        print(f\"  ✓ Test Accuracy: {test_acc_tuned:.4f}\")\n",
    "        print(f\"  ✓ Improvement: {tuning_results[model_name]['Improvement']:.4f}\")\n",
    "        print(f\"  ✓ Best Parameters: {grid_search.best_params_}\\n\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TUNING COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2824d8b",
   "metadata": {},
   "source": [
    "# Phase 3: Select and Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74713032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best overall model (tuned or untuned)\n",
    "all_scores = {}\n",
    "\n",
    "# Add default model scores\n",
    "for name, res in results.items():\n",
    "    all_scores[name] = {'Test Accuracy': res['Test Accuracy'], 'Tuned': False}\n",
    "\n",
    "# Add tuned model scores (will override if better)\n",
    "for name, res in tuning_results.items():\n",
    "    if res['Test Accuracy'] > all_scores[name]['Test Accuracy']:\n",
    "        all_scores[name] = {'Test Accuracy': res['Test Accuracy'], 'Tuned': True}\n",
    "\n",
    "# Find best model\n",
    "best_model_name = max(all_scores, key=lambda x: all_scores[x]['Test Accuracy'])\n",
    "best_accuracy = all_scores[best_model_name]['Test Accuracy']\n",
    "is_tuned = all_scores[best_model_name]['Tuned']\n",
    "\n",
    "# Get the best model pipeline\n",
    "if is_tuned and best_model_name in tuned_models:\n",
    "    best_model_pipeline = tuned_models[best_model_name]\n",
    "else:\n",
    "    best_model_pipeline = trained_models[best_model_name]\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BEST MODEL SELECTION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "print(f\"Test Accuracy: {best_accuracy:.4f}\")\n",
    "print(f\"Hyperparameter Tuned: {'Yes' if is_tuned else 'No'}\")\n",
    "\n",
    "if is_tuned and best_model_name in tuning_results:\n",
    "    print(f\"Best Parameters: {tuning_results[best_model_name]['Best Parameters']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DETAILED CLASSIFICATION REPORT\")\n",
    "print(\"=\"*70)\n",
    "y_pred_best = best_model_pipeline.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_best, target_names=['Low', 'Medium', 'High', 'Critical']))\n",
    "\n",
    "# Save the best model\n",
    "models_dir = '../models'\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "\n",
    "model_path = os.path.join(models_dir, 'best_model.pkl')\n",
    "with open(model_path, 'wb') as f:\n",
    "    pickle.dump(best_model_pipeline, f)\n",
    "\n",
    "# Also save model metadata\n",
    "metadata = {\n",
    "    'model_name': best_model_name,\n",
    "    'test_accuracy': best_accuracy,\n",
    "    'is_tuned': is_tuned,\n",
    "    'best_params': tuning_results[best_model_name]['Best Parameters'] if is_tuned and best_model_name in tuning_results else None\n",
    "}\n",
    "\n",
    "metadata_path = os.path.join(models_dir, 'model_metadata.pkl')\n",
    "with open(metadata_path, 'wb') as f:\n",
    "    pickle.dump(metadata, f)\n",
    "\n",
    "print(f\"\\n✓ Best model saved to {model_path}\")\n",
    "print(f\"✓ Model metadata saved to {metadata_path}\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
