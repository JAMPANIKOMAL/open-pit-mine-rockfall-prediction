{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf2303e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (fallback if not already installed)\n",
    "# Now includes advanced ML libraries for state-of-the-art modeling\n",
    "import sys\n",
    "!{sys.executable} -m pip install pandas numpy scikit-learn xgboost lightgbm shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689e9d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Logistic Regression ---\n",
      "Logistic Regression trained successfully.\n",
      "\n",
      "--- Training Random Forest ---\n",
      "Random Forest trained successfully.\n",
      "\n",
      "--- Training Support Vector Machine ---\n",
      "Support Vector Machine trained successfully.\n",
      "\n",
      "--- Evaluating Logistic Regression ---\n",
      "Accuracy: 0.9710\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Low       0.99      1.00      0.99       250\n",
      "      Medium       0.97      0.98      0.97       250\n",
      "        High       0.97      0.96      0.97       250\n",
      "    Critical       0.96      0.94      0.95       250\n",
      "\n",
      "    accuracy                           0.97      1000\n",
      "   macro avg       0.97      0.97      0.97      1000\n",
      "weighted avg       0.97      0.97      0.97      1000\n",
      "\n",
      "--- Evaluating Random Forest ---\n",
      "Accuracy: 0.9420\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Low       0.98      0.98      0.98       250\n",
      "      Medium       0.92      0.94      0.93       250\n",
      "        High       0.95      0.96      0.95       250\n",
      "    Critical       0.92      0.88      0.90       250\n",
      "\n",
      "    accuracy                           0.94      1000\n",
      "   macro avg       0.94      0.94      0.94      1000\n",
      "weighted avg       0.94      0.94      0.94      1000\n",
      "\n",
      "--- Evaluating Support Vector Machine ---\n",
      "Accuracy: 0.9820\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Low       0.99      1.00      0.99       250\n",
      "      Medium       0.99      0.98      0.99       250\n",
      "        High       0.98      0.97      0.98       250\n",
      "    Critical       0.96      0.98      0.97       250\n",
      "\n",
      "    accuracy                           0.98      1000\n",
      "   macro avg       0.98      0.98      0.98      1000\n",
      "weighted avg       0.98      0.98      0.98      1000\n",
      "\n",
      "\n",
      "Best performing model: Support Vector Machine with an accuracy of 0.9820\n",
      "Best model (Support Vector Machine) saved to ../models\\best_model.pkl\n"
     ]
    }
   ],
   "source": [
    "# Open-Pit Mine Rockfall Prediction - Advanced Model Development\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, StackingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import advanced ML libraries\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGBOOST_AVAILABLE = True\n",
    "except ImportError:\n",
    "    XGBOOST_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è XGBoost not available. Installing...\")\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    LIGHTGBM_AVAILABLE = True\n",
    "except ImportError:\n",
    "    LIGHTGBM_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è LightGBM not available. Installing...\")\n",
    "\n",
    "# Load the processed data\n",
    "processed_data_dir = '../data/processed'\n",
    "X_train = pd.read_csv(os.path.join(processed_data_dir, 'X_train.csv'))\n",
    "X_test = pd.read_csv(os.path.join(processed_data_dir, 'X_test.csv'))\n",
    "y_train = pd.read_csv(os.path.join(processed_data_dir, 'y_train.csv')).values.ravel()\n",
    "y_test = pd.read_csv(os.path.join(processed_data_dir, 'y_test.csv')).values.ravel()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"OPEN-PIT MINE ROCKFALL RISK PREDICTION - ADVANCED MODEL DEVELOPMENT\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nData loaded successfully!\")\n",
    "print(f\"  Training set: {X_train.shape[0]} samples, {X_train.shape[1]} features\")\n",
    "print(f\"  Test set: {X_test.shape[0]} samples, {X_test.shape[1]} features\")\n",
    "print(f\"\\nRisk Categories: {np.unique(y_train)}\")\n",
    "print(f\"  Training distribution: {dict(zip(*np.unique(y_train, return_counts=True)))}\")\n",
    "print(f\"  Test distribution: {dict(zip(*np.unique(y_test, return_counts=True)))}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561173a9",
   "metadata": {},
   "source": [
    "# Phase 1: Train Multiple Classification Models (Traditional + Advanced)\n",
    "\n",
    "We'll evaluate both traditional ML algorithms AND state-of-the-art gradient boosting methods to find the best model for mine rockfall prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6245a33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels for XGBoost and LightGBM (they need numeric labels)\n",
    "le = LabelEncoder()\n",
    "y_train_encoded = le.fit_transform(y_train)\n",
    "y_test_encoded = le.transform(y_test)\n",
    "label_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "print(f\"\\nLabel Encoding: {label_mapping}\")\n",
    "\n",
    "# Define models with default parameters\n",
    "models = {\n",
    "    'Logistic Regression': Pipeline([\n",
    "        ('scaler', StandardScaler()), \n",
    "        ('clf', LogisticRegression(random_state=42, max_iter=1000, multi_class='multinomial'))\n",
    "    ]),\n",
    "    'Random Forest': Pipeline([\n",
    "        ('scaler', StandardScaler()), \n",
    "        ('clf', RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1))\n",
    "    ]),\n",
    "    'Support Vector Machine': Pipeline([\n",
    "        ('scaler', StandardScaler()), \n",
    "        ('clf', SVC(random_state=42, probability=True))  # probability=True for ROC curves\n",
    "    ]),\n",
    "    'Decision Tree': Pipeline([\n",
    "        ('scaler', StandardScaler()), \n",
    "        ('clf', DecisionTreeClassifier(random_state=42, max_depth=20))\n",
    "    ]),\n",
    "    'Naive Bayes': Pipeline([\n",
    "        ('scaler', StandardScaler()), \n",
    "        ('clf', GaussianNB())\n",
    "    ]),\n",
    "    'K-Nearest Neighbors': Pipeline([\n",
    "        ('scaler', StandardScaler()), \n",
    "        ('clf', KNeighborsClassifier(n_neighbors=5))\n",
    "    ]),\n",
    "    'Gradient Boosting': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf', GradientBoostingClassifier(n_estimators=100, random_state=42))\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Add XGBoost if available\n",
    "if XGBOOST_AVAILABLE:\n",
    "    models['XGBoost'] = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf', xgb.XGBClassifier(\n",
    "            n_estimators=100,\n",
    "            random_state=42,\n",
    "            eval_metric='mlogloss',\n",
    "            use_label_encoder=False,\n",
    "            n_jobs=-1\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "# Add LightGBM if available\n",
    "if LIGHTGBM_AVAILABLE:\n",
    "    models['LightGBM'] = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf', lgb.LGBMClassifier(\n",
    "            n_estimators=100,\n",
    "            random_state=42,\n",
    "            verbose=-1,\n",
    "            n_jobs=-1\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "# Train and evaluate all models\n",
    "results = {}\n",
    "trained_models = {}\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 1: TRAINING AND EVALUATING ALL MODELS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Use stratified k-fold for better evaluation\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for name, pipeline in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    \n",
    "    # For XGBoost and LightGBM, use encoded labels\n",
    "    if name in ['XGBoost', 'LightGBM']:\n",
    "        y_train_use = y_train_encoded\n",
    "        y_test_use = y_test_encoded\n",
    "    else:\n",
    "        y_train_use = y_train\n",
    "        y_test_use = y_test\n",
    "    \n",
    "    # Train the model\n",
    "    pipeline.fit(X_train, y_train_use)\n",
    "    trained_models[name] = pipeline\n",
    "    \n",
    "    # Cross-validation score (stratified 5-fold)\n",
    "    cv_scores = cross_val_score(pipeline, X_train, y_train_use, cv=skf, scoring='accuracy', n_jobs=-1)\n",
    "    cv_mean = cv_scores.mean()\n",
    "    cv_std = cv_scores.std()\n",
    "    \n",
    "    # Test set evaluation\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test_use, y_pred)\n",
    "    test_f1 = f1_score(y_test_use, y_pred, average='weighted')\n",
    "    \n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        'CV Mean': cv_mean,\n",
    "        'CV Std': cv_std,\n",
    "        'Test Accuracy': test_accuracy,\n",
    "        'Test F1-Score': test_f1\n",
    "    }\n",
    "    \n",
    "    print(f\"  ‚úì Cross-Validation Accuracy: {cv_mean:.4f} (¬±{cv_std:.4f})\")\n",
    "    print(f\"  ‚úì Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"  ‚úì Test F1-Score: {test_f1:.4f}\\n\")\n",
    "\n",
    "# Display results summary\n",
    "results_df = pd.DataFrame(results).T.sort_values('Test Accuracy', ascending=False)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL COMPARISON SUMMARY (RANKED BY TEST ACCURACY)\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_string())\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Highlight top performer\n",
    "best_default_model = results_df.index[0]\n",
    "print(f\"\\nüèÜ Best Default Model: {best_default_model}\")\n",
    "print(f\"   Test Accuracy: {results_df.loc[best_default_model, 'Test Accuracy']:.4f}\")\n",
    "print(f\"   Test F1-Score: {results_df.loc[best_default_model, 'Test F1-Score']:.4f}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44575ef9",
   "metadata": {},
   "source": [
    "# Phase 2: Hyperparameter Tuning for Top Models\n",
    "\n",
    "We'll use GridSearchCV to optimize hyperparameters for the best performing models. This is crucial for maximizing predictive accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cecf411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top 4 models for hyperparameter tuning\n",
    "top_n = min(4, len(results_df))\n",
    "top_models = results_df.head(top_n).index.tolist()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 2: HYPERPARAMETER TUNING WITH GRIDSEARCHCV\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTuning top {len(top_models)} models: {', '.join(top_models)}\\n\")\n",
    "\n",
    "# Define parameter grids for each model type\n",
    "param_grids = {\n",
    "    'Random Forest': {\n",
    "        'clf__n_estimators': [100, 200, 300],\n",
    "        'clf__max_depth': [15, 20, 30, None],\n",
    "        'clf__min_samples_split': [2, 5, 10],\n",
    "        'clf__min_samples_leaf': [1, 2, 4],\n",
    "        'clf__max_features': ['sqrt', 'log2']\n",
    "    },\n",
    "    'Support Vector Machine': {\n",
    "        'clf__C': [0.1, 1, 10, 100],\n",
    "        'clf__gamma': ['scale', 'auto', 0.001, 0.01],\n",
    "        'clf__kernel': ['rbf', 'poly']\n",
    "    },\n",
    "    'Logistic Regression': {\n",
    "        'clf__C': [0.01, 0.1, 1, 10, 100],\n",
    "        'clf__penalty': ['l2'],\n",
    "        'clf__solver': ['lbfgs', 'saga', 'newton-cg']\n",
    "    },\n",
    "    'K-Nearest Neighbors': {\n",
    "        'clf__n_neighbors': [3, 5, 7, 9, 11, 15],\n",
    "        'clf__weights': ['uniform', 'distance'],\n",
    "        'clf__metric': ['euclidean', 'manhattan', 'minkowski']\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'clf__n_estimators': [100, 200, 300],\n",
    "        'clf__max_depth': [3, 5, 7, 10],\n",
    "        'clf__learning_rate': [0.01, 0.1, 0.3],\n",
    "        'clf__subsample': [0.8, 1.0],\n",
    "        'clf__colsample_bytree': [0.8, 1.0]\n",
    "    },\n",
    "    'LightGBM': {\n",
    "        'clf__n_estimators': [100, 200, 300],\n",
    "        'clf__max_depth': [3, 5, 7, 10],\n",
    "        'clf__learning_rate': [0.01, 0.1, 0.3],\n",
    "        'clf__subsample': [0.8, 1.0],\n",
    "        'clf__colsample_bytree': [0.8, 1.0]\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'clf__n_estimators': [100, 200, 300],\n",
    "        'clf__max_depth': [3, 5, 7],\n",
    "        'clf__learning_rate': [0.01, 0.1, 0.3],\n",
    "        'clf__subsample': [0.8, 1.0]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Perform grid search for top models\n",
    "tuned_models = {}\n",
    "tuning_results = {}\n",
    "\n",
    "for model_name in top_models:\n",
    "    if model_name in param_grids:\n",
    "        print(f\"Tuning {model_name}...\")\n",
    "        total_combinations = np.prod([len(v) for v in param_grids[model_name].values()])\n",
    "        print(f\"  Parameter grid size: {total_combinations} combinations\")\n",
    "        print(f\"  This may take several minutes...\")\n",
    "        \n",
    "        # Create a fresh pipeline\n",
    "        base_pipeline = models[model_name]\n",
    "        \n",
    "        # Determine which labels to use\n",
    "        if model_name in ['XGBoost', 'LightGBM']:\n",
    "            y_train_use = y_train_encoded\n",
    "            y_test_use = y_test_encoded\n",
    "        else:\n",
    "            y_train_use = y_train\n",
    "            y_test_use = y_test\n",
    "        \n",
    "        # Grid search with 3-fold stratified CV (faster than 5-fold)\n",
    "        grid_search = GridSearchCV(\n",
    "            base_pipeline,\n",
    "            param_grids[model_name],\n",
    "            cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=42),\n",
    "            scoring='accuracy',\n",
    "            n_jobs=-1,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        grid_search.fit(X_train, y_train_use)\n",
    "        \n",
    "        # Store tuned model\n",
    "        tuned_models[model_name] = grid_search.best_estimator_\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        y_pred_tuned = grid_search.predict(X_test)\n",
    "        test_acc_tuned = accuracy_score(y_test_use, y_pred_tuned)\n",
    "        test_f1_tuned = f1_score(y_test_use, y_pred_tuned, average='weighted')\n",
    "        \n",
    "        tuning_results[model_name] = {\n",
    "            'Best CV Score': grid_search.best_score_,\n",
    "            'Test Accuracy': test_acc_tuned,\n",
    "            'Test F1-Score': test_f1_tuned,\n",
    "            'Best Parameters': grid_search.best_params_,\n",
    "            'Accuracy Improvement': test_acc_tuned - results[model_name]['Test Accuracy'],\n",
    "            'F1 Improvement': test_f1_tuned - results[model_name]['Test F1-Score']\n",
    "        }\n",
    "        \n",
    "        print(f\"  ‚úì Best CV Score: {grid_search.best_score_:.4f}\")\n",
    "        print(f\"  ‚úì Test Accuracy: {test_acc_tuned:.4f} (Œî +{tuning_results[model_name]['Accuracy Improvement']:.4f})\")\n",
    "        print(f\"  ‚úì Test F1-Score: {test_f1_tuned:.4f} (Œî +{tuning_results[model_name]['F1 Improvement']:.4f})\")\n",
    "        print(f\"  ‚úì Best Parameters:\")\n",
    "        for param, value in grid_search.best_params_.items():\n",
    "            print(f\"      {param}: {value}\")\n",
    "        print()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"HYPERPARAMETER TUNING COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Display tuning results\n",
    "if tuning_results:\n",
    "    tuning_df = pd.DataFrame(tuning_results).T[['Best CV Score', 'Test Accuracy', 'Test F1-Score', 'Accuracy Improvement', 'F1 Improvement']]\n",
    "    print(\"\\nTuning Results Summary:\")\n",
    "    print(tuning_df.to_string())\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2824d8b",
   "metadata": {},
   "source": [
    "# Phase 3: Ensemble Methods (Voting & Stacking)\n",
    "\n",
    "Ensemble methods combine multiple models to achieve better predictive performance than any single model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01095ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 3: CREATING ENSEMBLE MODELS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Get top 3 tuned models for ensemble (or default if not tuned)\n",
    "ensemble_base_models = []\n",
    "ensemble_names = []\n",
    "\n",
    "for i, model_name in enumerate(top_models[:3]):\n",
    "    if model_name in tuned_models:\n",
    "        # Use tuned version\n",
    "        model_to_use = tuned_models[model_name]\n",
    "        ensemble_names.append(f\"{model_name}_tuned\")\n",
    "    else:\n",
    "        # Use default version\n",
    "        model_to_use = trained_models[model_name]\n",
    "        ensemble_names.append(model_name)\n",
    "    \n",
    "    # For ensemble, we need to extract the classifier (without pipeline wrapper)\n",
    "    if model_name in ['XGBoost', 'LightGBM']:\n",
    "        # These need encoded labels - we'll handle separately\n",
    "        continue\n",
    "    else:\n",
    "        ensemble_base_models.append((f'model_{i+1}', model_to_use))\n",
    "\n",
    "print(f\"Creating ensemble with: {ensemble_names[:len(ensemble_base_models)]}\\n\")\n",
    "\n",
    "# 1. Voting Classifier (Soft Voting - uses probability estimates)\n",
    "if len(ensemble_base_models) >= 2:\n",
    "    print(\"1. Training Voting Classifier (Soft Voting)...\")\n",
    "    voting_clf = VotingClassifier(\n",
    "        estimators=ensemble_base_models,\n",
    "        voting='soft'  # Uses predicted probabilities\n",
    "    )\n",
    "    \n",
    "    voting_clf.fit(X_train, y_train)\n",
    "    y_pred_voting = voting_clf.predict(X_test)\n",
    "    \n",
    "    voting_acc = accuracy_score(y_test, y_pred_voting)\n",
    "    voting_f1 = f1_score(y_test, y_pred_voting, average='weighted')\n",
    "    \n",
    "    print(f\"  ‚úì Voting Ensemble Accuracy: {voting_acc:.4f}\")\n",
    "    print(f\"  ‚úì Voting Ensemble F1-Score: {voting_f1:.4f}\\n\")\n",
    "    \n",
    "    results['Voting Ensemble'] = {\n",
    "        'CV Mean': None,\n",
    "        'CV Std': None,\n",
    "        'Test Accuracy': voting_acc,\n",
    "        'Test F1-Score': voting_f1\n",
    "    }\n",
    "    trained_models['Voting Ensemble'] = voting_clf\n",
    "\n",
    "# 2. Stacking Classifier (Meta-learner approach)\n",
    "if len(ensemble_base_models) >= 2:\n",
    "    print(\"2. Training Stacking Classifier...\")\n",
    "    print(\"   Base models: Top 3 models\")\n",
    "    print(\"   Meta-model: Logistic Regression\\n\")\n",
    "    \n",
    "    stacking_clf = StackingClassifier(\n",
    "        estimators=ensemble_base_models,\n",
    "        final_estimator=LogisticRegression(random_state=42, max_iter=1000),\n",
    "        cv=3,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    stacking_clf.fit(X_train, y_train)\n",
    "    y_pred_stacking = stacking_clf.predict(X_test)\n",
    "    \n",
    "    stacking_acc = accuracy_score(y_test, y_pred_stacking)\n",
    "    stacking_f1 = f1_score(y_test, y_pred_stacking, average='weighted')\n",
    "    \n",
    "    print(f\"  ‚úì Stacking Ensemble Accuracy: {stacking_acc:.4f}\")\n",
    "    print(f\"  ‚úì Stacking Ensemble F1-Score: {stacking_f1:.4f}\\n\")\n",
    "    \n",
    "    results['Stacking Ensemble'] = {\n",
    "        'CV Mean': None,\n",
    "        'CV Std': None,\n",
    "        'Test Accuracy': stacking_acc,\n",
    "        'Test F1-Score': stacking_f1\n",
    "    }\n",
    "    trained_models['Stacking Ensemble'] = stacking_clf\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ENSEMBLE MODELS CREATED\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74713032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best overall model (including tuned, ensemble, and default models)\n",
    "all_scores = {}\n",
    "\n",
    "# Add default model scores\n",
    "for name, res in results.items():\n",
    "    all_scores[name] = {\n",
    "        'Test Accuracy': res['Test Accuracy'], \n",
    "        'Test F1-Score': res['Test F1-Score'],\n",
    "        'Type': 'Default'\n",
    "    }\n",
    "\n",
    "# Add tuned model scores (will override if better)\n",
    "for name, res in tuning_results.items():\n",
    "    if res['Test Accuracy'] > all_scores[name]['Test Accuracy']:\n",
    "        all_scores[name] = {\n",
    "            'Test Accuracy': res['Test Accuracy'],\n",
    "            'Test F1-Score': res['Test F1-Score'],\n",
    "            'Type': 'Tuned'\n",
    "        }\n",
    "\n",
    "# Find best model\n",
    "best_model_name = max(all_scores, key=lambda x: all_scores[x]['Test Accuracy'])\n",
    "best_accuracy = all_scores[best_model_name]['Test Accuracy']\n",
    "best_f1 = all_scores[best_model_name]['Test F1-Score']\n",
    "model_type = all_scores[best_model_name]['Type']\n",
    "\n",
    "# Get the best model pipeline\n",
    "if model_type == 'Tuned' and best_model_name in tuned_models:\n",
    "    best_model_pipeline = tuned_models[best_model_name]\n",
    "else:\n",
    "    best_model_pipeline = trained_models[best_model_name]\n",
    "\n",
    "# Determine which labels to use for predictions\n",
    "if best_model_name in ['XGBoost', 'LightGBM']:\n",
    "    y_test_use = y_test_encoded\n",
    "    y_pred_best = best_model_pipeline.predict(X_test)\n",
    "    # Convert back to original labels for display\n",
    "    y_pred_display = le.inverse_transform(y_pred_best)\n",
    "    y_test_display = le.inverse_transform(y_test_use)\n",
    "else:\n",
    "    y_test_use = y_test\n",
    "    y_pred_best = best_model_pipeline.predict(X_test)\n",
    "    y_pred_display = y_pred_best\n",
    "    y_test_display = y_test_use\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 4: BEST MODEL SELECTION & EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nüèÜ BEST MODEL: {best_model_name}\")\n",
    "print(f\"   Model Type: {model_type}\")\n",
    "print(f\"   Test Accuracy: {best_accuracy:.4f}\")\n",
    "print(f\"   Test F1-Score (Weighted): {best_f1:.4f}\")\n",
    "\n",
    "if model_type == 'Tuned' and best_model_name in tuning_results:\n",
    "    print(f\"\\n   Optimized Hyperparameters:\")\n",
    "    for param, value in tuning_results[best_model_name]['Best Parameters'].items():\n",
    "        print(f\"      {param.replace('clf__', '')}: {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DETAILED CLASSIFICATION REPORT\")\n",
    "print(\"=\"*80)\n",
    "print(classification_report(\n",
    "    y_test_display, \n",
    "    y_pred_display, \n",
    "    target_names=['Critical', 'High', 'Low', 'Medium']  # Alphabetical order\n",
    "))\n",
    "\n",
    "# All models comparison\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL MODEL COMPARISON (ALL MODELS)\")\n",
    "print(\"=\"*80)\n",
    "final_results_df = pd.DataFrame(all_scores).T\n",
    "final_results_df = final_results_df.sort_values('Test Accuracy', ascending=False)\n",
    "print(final_results_df.to_string())\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Visualize model comparison\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Plot 1: Accuracy comparison\n",
    "plt.subplot(1, 2, 1)\n",
    "model_names = final_results_df.index\n",
    "accuracies = final_results_df['Test Accuracy']\n",
    "colors = ['gold' if name == best_model_name else 'steelblue' for name in model_names]\n",
    "\n",
    "bars = plt.barh(model_names, accuracies, color=colors, edgecolor='black')\n",
    "plt.xlabel('Test Accuracy', fontweight='bold', fontsize=12)\n",
    "plt.title('Model Performance Comparison\\n(Best Model Highlighted)', fontweight='bold', fontsize=14)\n",
    "plt.xlim([0.8, 1.0])  # Adjust based on your results\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar, acc) in enumerate(zip(bars, accuracies)):\n",
    "    plt.text(acc + 0.005, bar.get_y() + bar.get_height()/2, \n",
    "             f'{acc:.4f}', va='center', fontweight='bold', fontsize=9)\n",
    "\n",
    "# Plot 2: F1-Score comparison\n",
    "plt.subplot(1, 2, 2)\n",
    "f1_scores = final_results_df['Test F1-Score']\n",
    "\n",
    "bars2 = plt.barh(model_names, f1_scores, color=colors, edgecolor='black')\n",
    "plt.xlabel('Test F1-Score (Weighted)', fontweight='bold', fontsize=12)\n",
    "plt.title('F1-Score Comparison\\n(Best Model Highlighted)', fontweight='bold', fontsize=14)\n",
    "plt.xlim([0.8, 1.0])  # Adjust based on your results\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar, f1) in enumerate(zip(bars2, f1_scores)):\n",
    "    plt.text(f1 + 0.005, bar.get_y() + bar.get_height()/2, \n",
    "             f'{f1:.4f}', va='center', fontweight='bold', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the best model\n",
    "models_dir = '../models'\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "\n",
    "model_path = os.path.join(models_dir, 'best_model.pkl')\n",
    "with open(model_path, 'wb') as f:\n",
    "    pickle.dump(best_model_pipeline, f)\n",
    "\n",
    "# Also save label encoder if using XGBoost/LightGBM\n",
    "if best_model_name in ['XGBoost', 'LightGBM']:\n",
    "    le_path = os.path.join(models_dir, 'label_encoder.pkl')\n",
    "    with open(le_path, 'wb') as f:\n",
    "        pickle.dump(le, f)\n",
    "    print(f\"\\n‚úì Label encoder saved to {le_path}\")\n",
    "\n",
    "# Save model metadata\n",
    "metadata = {\n",
    "    'model_name': best_model_name,\n",
    "    'model_type': model_type,\n",
    "    'test_accuracy': best_accuracy,\n",
    "    'test_f1_score': best_f1,\n",
    "    'best_params': tuning_results[best_model_name]['Best Parameters'] if model_type == 'Tuned' and best_model_name in tuning_results else None,\n",
    "    'uses_encoded_labels': best_model_name in ['XGBoost', 'LightGBM'],\n",
    "    'label_mapping': label_mapping if best_model_name in ['XGBoost', 'LightGBM'] else None,\n",
    "    'feature_names': X_train.columns.tolist(),\n",
    "    'risk_categories': ['Low', 'Medium', 'High', 'Critical']\n",
    "}\n",
    "\n",
    "metadata_path = os.path.join(models_dir, 'model_metadata.pkl')\n",
    "with open(metadata_path, 'wb') as f:\n",
    "    pickle.dump(metadata, f)\n",
    "\n",
    "print(f\"\\n‚úì Best model saved to {model_path}\")\n",
    "print(f\"‚úì Model metadata saved to {metadata_path}\")\n",
    "\n",
    "# Save all models for comparison in visualization notebook\n",
    "all_models_path = os.path.join(models_dir, 'all_models.pkl')\n",
    "with open(all_models_path, 'wb') as f:\n",
    "    pickle.dump(trained_models, f)\n",
    "print(f\"‚úì All models saved to {all_models_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL DEVELOPMENT COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nKey Achievements:\")\n",
    "print(f\"  ‚úì Trained {len(models)} different models\")\n",
    "print(f\"  ‚úì Hyperparameter tuning on top {len(tuning_results)} models\")\n",
    "print(f\"  ‚úì Created {2 if 'Voting Ensemble' in results and 'Stacking Ensemble' in results else 0} ensemble models\")\n",
    "print(f\"  ‚úì Best model: {best_model_name} ({best_accuracy:.4f} accuracy)\")\n",
    "print(f\"\\nNext steps:\")\n",
    "print(f\"  ‚Üí Run notebook 04 for comprehensive visualizations and SHAP analysis\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
