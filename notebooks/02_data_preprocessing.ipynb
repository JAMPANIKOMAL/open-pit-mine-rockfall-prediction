{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17fd7739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Labels: ['Critical' 'High' 'Low' 'Medium']\n",
      "Encoded Labels: [0 1 2 3]\n",
      "\n",
      "Shape of training features: (4000, 5)\n",
      "Shape of testing features: (1000, 5)\n",
      "\n",
      "Processed data saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# rockfall-prediction-system/notebooks/02_data_preprocessing.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('../data/rockfall_data.csv')\n",
    "\n",
    "# Separate Features (X) and Target (y)\n",
    "X = df.drop('rockfall_risk', axis=1)\n",
    "y = df['rockfall_risk']\n",
    "\n",
    "# Encode the Target Variable\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "print(\"Original Labels:\", label_encoder.classes_)\n",
    "print(\"Encoded Labels:\", np.unique(y_encoded))\n",
    "\n",
    "\n",
    "# Split Data into Training and Testing Sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, \n",
    "    y_encoded, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y_encoded\n",
    ")\n",
    "print(\"\\nShape of training features:\", X_train.shape)\n",
    "print(\"Shape of testing features:\", X_test.shape)\n",
    "\n",
    "# Save Processed Data\n",
    "processed_data_dir = '../data/processed'\n",
    "if not os.path.exists(processed_data_dir):\n",
    "    os.makedirs(processed_data_dir)\n",
    "\n",
    "X_train.to_csv(os.path.join(processed_data_dir, 'X_train.csv'), index=False)\n",
    "X_test.to_csv(os.path.join(processed_data_dir, 'X_test.csv'), index=False)\n",
    "pd.Series(y_train).to_csv(os.path.join(processed_data_dir, 'y_train.csv'), index=False, header=['rockfall_risk'])\n",
    "pd.Series(y_test).to_csv(os.path.join(processed_data_dir, 'y_test.csv'), index=False, header=['rockfall_risk'])\n",
    "\n",
    "print(\"\\nProcessed data saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c027dc",
   "metadata": {},
   "source": [
    "# Feature Scaling Discussion\n",
    "\n",
    "Before we proceed with model training, let's discuss why feature scaling is important and compare different scaling methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6634db88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why Feature Scaling is Important\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"IMPORTANCE OF FEATURE SCALING\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n1. DISTANCE-BASED ALGORITHMS (SVM, KNN):\")\n",
    "print(\"   - These algorithms use distances between data points\")\n",
    "print(\"   - Features with larger scales dominate the distance calculation\")\n",
    "print(\"   - Example: joint_water_pressure (50-450) vs seismic_activity (0.01-1.6)\")\n",
    "print(\"   - Without scaling, water pressure would dominate predictions\")\n",
    "\n",
    "print(\"\\n2. GRADIENT DESCENT OPTIMIZATION (Logistic Regression, Neural Networks):\")\n",
    "print(\"   - Features on different scales cause slow/unstable convergence\")\n",
    "print(\"   - Scaling helps the optimization algorithm converge faster\")\n",
    "\n",
    "print(\"\\n3. ALGORITHMS THAT DON'T REQUIRE SCALING:\")\n",
    "print(\"   - Tree-based models (Random Forest, Decision Tree)\")\n",
    "print(\"   - These use splits, not distances, so scale doesn't matter\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SCALING METHODS COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nStandardScaler (Z-score normalization):\")\n",
    "print(\"  Formula: z = (x - mean) / std_dev\")\n",
    "print(\"  Result: Mean=0, Std=1, range typically [-3, 3]\")\n",
    "print(\"  Best for: Data with Gaussian distribution, presence of outliers\")\n",
    "\n",
    "print(\"\\nMinMaxScaler:\")\n",
    "print(\"  Formula: x_scaled = (x - min) / (max - min)\")\n",
    "print(\"  Result: Range [0, 1] (or custom range)\")\n",
    "print(\"  Best for: Bounded features, when you need a specific range\")\n",
    "print(\"  Sensitive to: Outliers (they can compress the majority of data)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DECISION: We will use StandardScaler in our pipelines because:\")\n",
    "print(\"  1. Our data contains outliers (high-risk scenarios)\")\n",
    "print(\"  2. MinMaxScaler would compress most values due to extreme outliers\")\n",
    "print(\"  3. Works well with SVM and Logistic Regression\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73addefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Quick Comparison - StandardScaler vs MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SCALING COMPARISON EXPERIMENT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Test with SVM (sensitive to scaling)\n",
    "# StandardScaler\n",
    "scaler_standard = StandardScaler()\n",
    "X_train_standard = scaler_standard.fit_transform(X_train)\n",
    "X_test_standard = scaler_standard.transform(X_test)\n",
    "\n",
    "svm_standard = SVC(random_state=42)\n",
    "svm_standard.fit(X_train_standard, y_train)\n",
    "y_pred_standard = svm_standard.predict(X_test_standard)\n",
    "acc_standard = accuracy_score(y_test, y_pred_standard)\n",
    "\n",
    "# MinMaxScaler\n",
    "scaler_minmax = MinMaxScaler()\n",
    "X_train_minmax = scaler_minmax.fit_transform(X_train)\n",
    "X_test_minmax = scaler_minmax.transform(X_test)\n",
    "\n",
    "svm_minmax = SVC(random_state=42)\n",
    "svm_minmax.fit(X_train_minmax, y_train)\n",
    "y_pred_minmax = svm_minmax.predict(X_test_minmax)\n",
    "acc_minmax = accuracy_score(y_test, y_pred_minmax)\n",
    "\n",
    "print(f\"\\nSVM with StandardScaler: {acc_standard:.4f}\")\n",
    "print(f\"SVM with MinMaxScaler:    {acc_minmax:.4f}\")\n",
    "print(f\"\\nDifference: {abs(acc_standard - acc_minmax):.4f}\")\n",
    "\n",
    "if acc_standard > acc_minmax:\n",
    "    print(\"\\n✓ StandardScaler performs better (as expected with outliers)\")\n",
    "elif acc_minmax > acc_standard:\n",
    "    print(\"\\n✓ MinMaxScaler performs better (surprising given outliers!)\")\n",
    "else:\n",
    "    print(\"\\n✓ Both scalers perform equally well\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
