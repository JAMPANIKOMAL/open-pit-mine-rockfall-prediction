{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be7fe240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\jampa\\documents\\rockfall-prediction-system\\.venv\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\jampa\\documents\\rockfall-prediction-system\\.venv\\lib\\site-packages (2.3.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\jampa\\documents\\rockfall-prediction-system\\.venv\\lib\\site-packages (1.7.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\jampa\\documents\\rockfall-prediction-system\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jampa\\documents\\rockfall-prediction-system\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\jampa\\documents\\rockfall-prediction-system\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\jampa\\documents\\rockfall-prediction-system\\.venv\\lib\\site-packages (from scikit-learn) (1.16.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\jampa\\documents\\rockfall-prediction-system\\.venv\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\jampa\\documents\\rockfall-prediction-system\\.venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jampa\\documents\\rockfall-prediction-system\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "# Install required packages (fallback if not already installed)\n",
    "import sys\n",
    "!{sys.executable} -m pip install pandas numpy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17fd7739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DATA PREPROCESSING - COMBINED DATASET\n",
      "======================================================================\n",
      "\n",
      "✓ Loaded dataset: 20000 rows, 7 columns\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "STEP 1: Handling Missing Values\n",
      "----------------------------------------------------------------------\n",
      "Total missing values: 50000\n",
      "✓ Dropped 'data_source' column\n",
      "\n",
      "Features shape: (20000, 5)\n",
      "Target shape: (20000,)\n",
      "\n",
      "Applying median imputation for missing values...\n",
      "✓ Missing values after imputation: 0\n",
      "\n",
      "Imputation Summary:\n",
      "  - seismic_activity: 10000 values imputed with median = 0.26\n",
      "  - vibration_level: 10000 values imputed with median = 1.73\n",
      "  - joint_water_pressure: 10000 values imputed with median = 205.53\n",
      "  - displacement_mm: 10000 values imputed with median = 5.99\n",
      "  - rainfall_mm: 10000 values imputed with median = 27.76\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "STEP 2: Encoding Target Variable\n",
      "----------------------------------------------------------------------\n",
      "Original Labels: ['Critical' 'High' 'Low' 'Medium']\n",
      "Encoded Labels: [0 1 2 3]\n",
      "\n",
      "Encoding Mapping:\n",
      "  Critical → 0\n",
      "  High → 1\n",
      "  Low → 2\n",
      "  Medium → 3\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "STEP 3: Train-Test Split\n",
      "----------------------------------------------------------------------\n",
      "✓ Training set: 16000 samples\n",
      "✓ Testing set: 4000 samples\n",
      "✓ Number of features: 5\n",
      "\n",
      "Class distribution in training set:\n",
      "  Critical: 4000 (25.0%)\n",
      "  High: 4000 (25.0%)\n",
      "  Low: 4000 (25.0%)\n",
      "  Medium: 4000 (25.0%)\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "STEP 4: Saving Processed Data\n",
      "----------------------------------------------------------------------\n",
      "✓ X_train.csv saved\n",
      "✓ X_test.csv saved\n",
      "✓ y_train.csv saved\n",
      "✓ y_test.csv saved\n",
      "\n",
      "======================================================================\n",
      "PREPROCESSING COMPLETE\n",
      "======================================================================\n",
      "Ready for model training with 5 features!\n"
     ]
    }
   ],
   "source": [
    "# rockfall-prediction-system/notebooks/02_data_preprocessing.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"DATA PREPROCESSING - COMBINED DATASET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load the combined dataset\n",
    "df = pd.read_csv('../data/rockfall_data.csv')\n",
    "\n",
    "print(f\"\\n✓ Loaded dataset: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "\n",
    "# ========================================================================\n",
    "# Step 1: Handle Missing Values\n",
    "# ========================================================================\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"STEP 1: Handling Missing Values\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Check missing values\n",
    "missing_before = df.isnull().sum().sum()\n",
    "print(f\"Total missing values: {missing_before}\")\n",
    "\n",
    "# Drop 'data_source' column (not a feature)\n",
    "if 'data_source' in df.columns:\n",
    "    df = df.drop('data_source', axis=1)\n",
    "    print(\"✓ Dropped 'data_source' column\")\n",
    "\n",
    "# Separate Features (X) and Target (y)\n",
    "X = df.drop('rockfall_risk', axis=1)\n",
    "y = df['rockfall_risk']\n",
    "\n",
    "print(f\"\\nFeatures shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "\n",
    "# Strategy: Impute missing values with median (robust to outliers)\n",
    "print(\"\\nApplying median imputation for missing values...\")\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_imputed = pd.DataFrame(\n",
    "    imputer.fit_transform(X),\n",
    "    columns=X.columns,\n",
    "    index=X.index\n",
    ")\n",
    "\n",
    "missing_after = X_imputed.isnull().sum().sum()\n",
    "print(f\"✓ Missing values after imputation: {missing_after}\")\n",
    "\n",
    "print(\"\\nImputation Summary:\")\n",
    "for col in X.columns:\n",
    "    missing_count = X[col].isnull().sum()\n",
    "    if missing_count > 0:\n",
    "        median_value = X[col].median()\n",
    "        print(f\"  - {col}: {missing_count} values imputed with median = {median_value:.2f}\")\n",
    "\n",
    "# ========================================================================\n",
    "# Step 2: Encode Target Variable\n",
    "# ========================================================================\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"STEP 2: Encoding Target Variable\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "print(\"Original Labels:\", label_encoder.classes_)\n",
    "print(\"Encoded Labels:\", np.unique(y_encoded))\n",
    "print(\"\\nEncoding Mapping:\")\n",
    "for idx, label in enumerate(label_encoder.classes_):\n",
    "    print(f\"  {label} → {idx}\")\n",
    "\n",
    "# ========================================================================\n",
    "# Step 3: Train-Test Split\n",
    "# ========================================================================\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"STEP 3: Train-Test Split\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_imputed, \n",
    "    y_encoded, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y_encoded\n",
    ")\n",
    "\n",
    "print(f\"✓ Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"✓ Testing set: {X_test.shape[0]} samples\")\n",
    "print(f\"✓ Number of features: {X_train.shape[1]}\")\n",
    "\n",
    "print(\"\\nClass distribution in training set:\")\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "for label_idx, count in zip(unique, counts):\n",
    "    label_name = label_encoder.classes_[label_idx]\n",
    "    percentage = (count / len(y_train)) * 100\n",
    "    print(f\"  {label_name}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "# ========================================================================\n",
    "# Step 4: Save Processed Data\n",
    "# ========================================================================\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"STEP 4: Saving Processed Data\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "processed_data_dir = '../data/processed'\n",
    "if not os.path.exists(processed_data_dir):\n",
    "    os.makedirs(processed_data_dir)\n",
    "\n",
    "X_train.to_csv(os.path.join(processed_data_dir, 'X_train.csv'), index=False)\n",
    "X_test.to_csv(os.path.join(processed_data_dir, 'X_test.csv'), index=False)\n",
    "pd.Series(y_train).to_csv(os.path.join(processed_data_dir, 'y_train.csv'), index=False, header=['rockfall_risk'])\n",
    "pd.Series(y_test).to_csv(os.path.join(processed_data_dir, 'y_test.csv'), index=False, header=['rockfall_risk'])\n",
    "\n",
    "print(\"✓ X_train.csv saved\")\n",
    "print(\"✓ X_test.csv saved\")\n",
    "print(\"✓ y_train.csv saved\")\n",
    "print(\"✓ y_test.csv saved\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PREPROCESSING COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Ready for model training with {X_train.shape[1]} features!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c027dc",
   "metadata": {},
   "source": [
    "# Feature Scaling Discussion\n",
    "\n",
    "Before we proceed with model training, let's discuss why feature scaling is important and compare different scaling methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6634db88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "IMPORTANCE OF FEATURE SCALING\n",
      "======================================================================\n",
      "\n",
      "1. DISTANCE-BASED ALGORITHMS (SVM, KNN):\n",
      "   - These algorithms use distances between data points\n",
      "   - Features with larger scales dominate the distance calculation\n",
      "   - Example: joint_water_pressure (50-450) vs seismic_activity (0.01-1.6)\n",
      "   - Without scaling, water pressure would dominate predictions\n",
      "\n",
      "2. GRADIENT DESCENT OPTIMIZATION (Logistic Regression, Neural Networks):\n",
      "   - Features on different scales cause slow/unstable convergence\n",
      "   - Scaling helps the optimization algorithm converge faster\n",
      "\n",
      "3. ALGORITHMS THAT DON'T REQUIRE SCALING:\n",
      "   - Tree-based models (Random Forest, Decision Tree)\n",
      "   - These use splits, not distances, so scale doesn't matter\n",
      "\n",
      "======================================================================\n",
      "SCALING METHODS COMPARISON\n",
      "======================================================================\n",
      "\n",
      "StandardScaler (Z-score normalization):\n",
      "  Formula: z = (x - mean) / std_dev\n",
      "  Result: Mean=0, Std=1, range typically [-3, 3]\n",
      "  Best for: Data with Gaussian distribution, presence of outliers\n",
      "\n",
      "MinMaxScaler:\n",
      "  Formula: x_scaled = (x - min) / (max - min)\n",
      "  Result: Range [0, 1] (or custom range)\n",
      "  Best for: Bounded features, when you need a specific range\n",
      "  Sensitive to: Outliers (they can compress the majority of data)\n",
      "\n",
      "======================================================================\n",
      "DECISION: We will use StandardScaler in our pipelines because:\n",
      "  1. Our data contains outliers (high-risk scenarios)\n",
      "  2. MinMaxScaler would compress most values due to extreme outliers\n",
      "  3. Works well with SVM and Logistic Regression\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Why Feature Scaling is Important\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"IMPORTANCE OF FEATURE SCALING\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n1. DISTANCE-BASED ALGORITHMS (SVM, KNN):\")\n",
    "print(\"   - These algorithms use distances between data points\")\n",
    "print(\"   - Features with larger scales dominate the distance calculation\")\n",
    "print(\"   - Example: joint_water_pressure (50-450) vs seismic_activity (0.01-1.6)\")\n",
    "print(\"   - Without scaling, water pressure would dominate predictions\")\n",
    "\n",
    "print(\"\\n2. GRADIENT DESCENT OPTIMIZATION (Logistic Regression, Neural Networks):\")\n",
    "print(\"   - Features on different scales cause slow/unstable convergence\")\n",
    "print(\"   - Scaling helps the optimization algorithm converge faster\")\n",
    "\n",
    "print(\"\\n3. ALGORITHMS THAT DON'T REQUIRE SCALING:\")\n",
    "print(\"   - Tree-based models (Random Forest, Decision Tree)\")\n",
    "print(\"   - These use splits, not distances, so scale doesn't matter\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SCALING METHODS COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nStandardScaler (Z-score normalization):\")\n",
    "print(\"  Formula: z = (x - mean) / std_dev\")\n",
    "print(\"  Result: Mean=0, Std=1, range typically [-3, 3]\")\n",
    "print(\"  Best for: Data with Gaussian distribution, presence of outliers\")\n",
    "\n",
    "print(\"\\nMinMaxScaler:\")\n",
    "print(\"  Formula: x_scaled = (x - min) / (max - min)\")\n",
    "print(\"  Result: Range [0, 1] (or custom range)\")\n",
    "print(\"  Best for: Bounded features, when you need a specific range\")\n",
    "print(\"  Sensitive to: Outliers (they can compress the majority of data)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DECISION: We will use StandardScaler in our pipelines because:\")\n",
    "print(\"  1. Our data contains outliers (high-risk scenarios)\")\n",
    "print(\"  2. MinMaxScaler would compress most values due to extreme outliers\")\n",
    "print(\"  3. Works well with SVM and Logistic Regression\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73addefa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SCALING COMPARISON EXPERIMENT\n",
      "======================================================================\n",
      "\n",
      "SVM with StandardScaler: 0.6192\n",
      "SVM with MinMaxScaler:    0.6192\n",
      "\n",
      "Difference: 0.0000\n",
      "\n",
      "✓ Both scalers perform equally well\n",
      "\n",
      "======================================================================\n",
      "This experiment validates our choice of StandardScaler for the model pipelines.\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Optional: Quick Comparison - StandardScaler vs MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SCALING COMPARISON EXPERIMENT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Test with SVM (sensitive to scaling)\n",
    "# StandardScaler\n",
    "scaler_standard = StandardScaler()\n",
    "X_train_standard = scaler_standard.fit_transform(X_train)\n",
    "X_test_standard = scaler_standard.transform(X_test)\n",
    "\n",
    "svm_standard = SVC(random_state=42)\n",
    "svm_standard.fit(X_train_standard, y_train)\n",
    "y_pred_standard = svm_standard.predict(X_test_standard)\n",
    "acc_standard = accuracy_score(y_test, y_pred_standard)\n",
    "\n",
    "# MinMaxScaler\n",
    "scaler_minmax = MinMaxScaler()\n",
    "X_train_minmax = scaler_minmax.fit_transform(X_train)\n",
    "X_test_minmax = scaler_minmax.transform(X_test)\n",
    "\n",
    "svm_minmax = SVC(random_state=42)\n",
    "svm_minmax.fit(X_train_minmax, y_train)\n",
    "y_pred_minmax = svm_minmax.predict(X_test_minmax)\n",
    "acc_minmax = accuracy_score(y_test, y_pred_minmax)\n",
    "\n",
    "print(f\"\\nSVM with StandardScaler: {acc_standard:.4f}\")\n",
    "print(f\"SVM with MinMaxScaler:    {acc_minmax:.4f}\")\n",
    "print(f\"\\nDifference: {abs(acc_standard - acc_minmax):.4f}\")\n",
    "\n",
    "if acc_standard > acc_minmax:\n",
    "    print(\"\\n✓ StandardScaler performs better (as expected with our diverse feature ranges)\")\n",
    "elif acc_minmax > acc_standard:\n",
    "    print(\"\\n✓ MinMaxScaler performs better\")\n",
    "else:\n",
    "    print(\"\\n✓ Both scalers perform equally well\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"This experiment validates our choice of StandardScaler for the model pipelines.\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
