{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15811b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle in c:\\users\\jampa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.7.4.5)\n",
      "Requirement already satisfied: pandas in c:\\users\\jampa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\jampa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.2.6)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\jampa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.10.7)\n",
      "Requirement already satisfied: seaborn in c:\\users\\jampa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\jampa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.7.2)\n",
      "Requirement already satisfied: bleach in c:\\users\\jampa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from kaggle) (6.3.0)\n",
      "Requirement already satisfied: certifi>=14.05.14 in c:\\users\\jampa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from kaggle) (2025.8.3)\n",
      "Requirement already satisfied: charset-normalizer in c:\\users\\jampa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from kaggle) (3.4.3)\n",
      "Requirement already satisfied: idna in c:\\users\\jampa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from kaggle) (3.10)\n",
      "Requirement already satisfied: protobuf in c:\\users\\jampa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from kaggle) (6.33.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\jampa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from kaggle) (2.9.0.post0)\n",
      "Requirement already satisfied: python-slugify in c:\\users\\jampa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from kaggle) (8.0.4)\n",
      "Requirement already satisfied: requests in c:\\users\\jampa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from kaggle) (2.32.5)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in c:\\users\\jampa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from kaggle) (80.9.0)\n",
      "Requirement already satisfied: six>=1.10 in c:\\users\\jampa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from kaggle) (1.17.0)\n",
      "Requirement already satisfied: text-unidecode in c:\\users\\jampa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from kaggle) (1.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\jampa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from kaggle) (4.67.1)\n",
      "Requirement already satisfied: urllib3>=1.15.1 in c:\\users\\jampa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from kaggle) (2.5.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\jampa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from kaggle) (0.5.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jampa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\jampa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\jampa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\jampa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\jampa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\jampa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jampa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\jampa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\jampa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\jampa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (1.16.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\jampa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\jampa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\jampa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tqdm->kaggle) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "# Install required packages (fallback if not already installed)\n",
    "# Note: opendatasets has compatibility issues with Python 3.13, so we use kaggle CLI instead\n",
    "import sys\n",
    "!{sys.executable} -m pip install kaggle pandas numpy matplotlib seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07f770f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ROCKFALL RISK PREDICTION - DATA GENERATION & INTEGRATION\n",
      "======================================================================\n",
      "\n",
      "[PART A] Generating Synthetic Rockfall Sensor Data...\n",
      "----------------------------------------------------------------------\n",
      "âœ“ Synthetic data generated: 10000 samples\n",
      "  Features: ['seismic_activity', 'vibration_level', 'joint_water_pressure', 'displacement_mm', 'rainfall_mm']\n",
      "\n",
      "[PART B] Downloading Kaggle Landslide Dataset...\n",
      "----------------------------------------------------------------------\n",
      "Downloading from Kaggle: snehilmathur/landslide-dataset-for-classification\n",
      "NOTE: Make sure you have set up Kaggle API credentials.\n",
      "If you haven't, see README for setup instructions.\n",
      "\n",
      "Dataset URL: https://www.kaggle.com/datasets/snehilmathur/landslide-dataset-for-classification\n",
      "Error downloading dataset: 403 Client Error: Forbidden for url: https://www.kaggle.com/api/v1/datasets/download/snehilmathur/landslide-dataset-for-classification?raw=false\n",
      "\n",
      "Troubleshooting:\n",
      "1. Make sure kaggle.json is in C:\\Users\\jampa\\.kaggle\\\n",
      "2. Run: pip install kaggle\n",
      "3. Check your internet connection\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "403 Client Error: Forbidden for url: https://www.kaggle.com/api/v1/datasets/download/snehilmathur/landslide-dataset-for-classification?raw=false",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 88\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     87\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkaggle\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m     \u001b[43mkaggle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdataset_download_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munzip\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mâœ“ Dataset downloaded successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\kaggle\\api\\kaggle_api_extended.py:1664\u001b[39m, in \u001b[36mKaggleApi.dataset_download_files\u001b[39m\u001b[34m(self, dataset, path, force, quiet, unzip, licenses)\u001b[39m\n\u001b[32m   1662\u001b[39m request.dataset_slug = dataset_slug\n\u001b[32m   1663\u001b[39m request.dataset_version_number = dataset_version_number\n\u001b[32m-> \u001b[39m\u001b[32m1664\u001b[39m response = \u001b[43mkaggle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdataset_api_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdownload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1666\u001b[39m outfile = os.path.join(effective_path, dataset_slug + \u001b[33m'\u001b[39m\u001b[33m.zip\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m   1667\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m force \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.download_needed(response, outfile, quiet):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\kagglesdk\\datasets\\services\\dataset_api_service.py:80\u001b[39m, in \u001b[36mDatasetApiClient.download_dataset\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m request \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     78\u001b[39m   request = ApiDownloadDatasetRequest()\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdatasets.DatasetApiService\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mApiDownloadDataset\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mHttpRedirect\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\kagglesdk\\kaggle_http_client.py:126\u001b[39m, in \u001b[36mKaggleHttpClient.call\u001b[39m\u001b[34m(self, service_name, request_name, request, response_type)\u001b[39m\n\u001b[32m    123\u001b[39m settings = \u001b[38;5;28mself\u001b[39m._session.merge_environment_settings(http_request.url, {}, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    124\u001b[39m http_response = \u001b[38;5;28mself\u001b[39m._session.send(http_request, **settings)\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_response\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\kagglesdk\\kaggle_http_client.py:191\u001b[39m, in \u001b[36mKaggleHttpClient._prepare_response\u001b[39m\u001b[34m(self, response_type, http_response)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_prepare_response\u001b[39m(\u001b[38;5;28mself\u001b[39m, response_type, http_response):\n\u001b[32m    190\u001b[39m   \u001b[38;5;28mself\u001b[39m._print_response(http_response)\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m   \u001b[43mhttp_response\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    192\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mapplication/json\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m http_response.headers[\u001b[33m'\u001b[39m\u001b[33mContent-Type\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m    193\u001b[39m     resp = http_response.json()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\requests\\models.py:1026\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1021\u001b[39m     http_error_msg = (\n\u001b[32m   1022\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1023\u001b[39m     )\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 403 Client Error: Forbidden for url: https://www.kaggle.com/api/v1/datasets/download/snehilmathur/landslide-dataset-for-classification?raw=false"
     ]
    }
   ],
   "source": [
    "# 1. Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import subprocess\n",
    "\n",
    "# 2. Setup Directories\n",
    "if not os.path.exists('../data'):\n",
    "    os.makedirs('../data')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ROCKFALL RISK PREDICTION - DATA GENERATION & INTEGRATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ========================================================================\n",
    "# PART A: Generate Synthetic Rockfall Sensor Data\n",
    "# ========================================================================\n",
    "print(\"\\n[PART A] Generating Synthetic Rockfall Sensor Data...\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "num_samples = 10000  # Increased from 5000 to 10000\n",
    "np.random.seed(42)\n",
    "\n",
    "# Base values for stable conditions\n",
    "synthetic_data = {\n",
    "    'seismic_activity': np.random.uniform(0.01, 0.2, num_samples),\n",
    "    'vibration_level': np.random.uniform(0.1, 1.5, num_samples),\n",
    "    'joint_water_pressure': np.random.uniform(50, 200, num_samples),\n",
    "    'displacement_mm': np.random.uniform(0.0, 2.0, num_samples),\n",
    "    'rainfall_mm': np.random.uniform(0, 10, num_samples)\n",
    "}\n",
    "\n",
    "df_synthetic = pd.DataFrame(synthetic_data)\n",
    "\n",
    "# Introduce conditions that lead to higher risk\n",
    "high_risk_indices = np.random.choice(df_synthetic.index, size=int(num_samples * 0.1), replace=False)\n",
    "df_synthetic.loc[high_risk_indices, 'seismic_activity'] *= np.random.uniform(3, 8)\n",
    "df_synthetic.loc[high_risk_indices, 'vibration_level'] *= np.random.uniform(2, 5)\n",
    "df_synthetic.loc[high_risk_indices, 'joint_water_pressure'] += np.random.uniform(100, 250)\n",
    "df_synthetic.loc[high_risk_indices, 'displacement_mm'] += np.random.uniform(3, 10)\n",
    "df_synthetic.loc[high_risk_indices, 'rainfall_mm'] += np.random.uniform(20, 50)\n",
    "\n",
    "medium_risk_indices = np.random.choice(df_synthetic.drop(high_risk_indices).index, size=int(num_samples * 0.2), replace=False)\n",
    "df_synthetic.loc[medium_risk_indices, 'seismic_activity'] *= np.random.uniform(1.5, 3)\n",
    "df_synthetic.loc[medium_risk_indices, 'vibration_level'] *= np.random.uniform(1.5, 3)\n",
    "df_synthetic.loc[medium_risk_indices, 'joint_water_pressure'] += np.random.uniform(50, 150)\n",
    "df_synthetic.loc[medium_risk_indices, 'displacement_mm'] += np.random.uniform(1, 4)\n",
    "df_synthetic.loc[medium_risk_indices, 'rainfall_mm'] += np.random.uniform(10, 30)\n",
    "\n",
    "# Define risk based on a scoring system\n",
    "score = (\n",
    "    df_synthetic['seismic_activity'] * 2.0 + \n",
    "    df_synthetic['vibration_level'] * 1.5 + \n",
    "    df_synthetic['joint_water_pressure'] * 0.05 + \n",
    "    df_synthetic['displacement_mm'] * 1.0 + \n",
    "    df_synthetic['rainfall_mm'] * 0.2\n",
    ")\n",
    "\n",
    "labels = ['Low', 'Medium', 'High', 'Critical']\n",
    "df_synthetic['rockfall_risk'] = pd.qcut(score, q=4, labels=labels, duplicates='drop')\n",
    "df_synthetic.dropna(subset=['rockfall_risk'], inplace=True)\n",
    "df_synthetic['data_source'] = 'Synthetic'\n",
    "\n",
    "print(f\"âœ“ Synthetic data generated: {len(df_synthetic)} samples\")\n",
    "print(f\"  Features: {list(df_synthetic.columns[:-2])}\")\n",
    "\n",
    "# ========================================================================\n",
    "# PART B: Download Kaggle Landslide Dataset\n",
    "# ========================================================================\n",
    "print(\"\\n[PART B] Downloading Kaggle Landslide Dataset...\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "dataset_name = 'snehilmathur/landslide-dataset-for-classification'\n",
    "download_dir = '../data/kaggle_landslide'\n",
    "\n",
    "print(f\"Downloading from Kaggle: {dataset_name}\")\n",
    "print(\"NOTE: Make sure you have set up Kaggle API credentials.\")\n",
    "print(\"If you haven't, see README for setup instructions.\\n\")\n",
    "\n",
    "# Create download directory\n",
    "os.makedirs(download_dir, exist_ok=True)\n",
    "\n",
    "# Download dataset using Kaggle CLI\n",
    "try:\n",
    "    import kaggle\n",
    "    kaggle.api.dataset_download_files(dataset_name, path=download_dir, unzip=True)\n",
    "    print(\"âœ“ Dataset downloaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading dataset: {e}\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"1. Make sure kaggle.json is in C:\\\\Users\\\\jampa\\\\.kaggle\\\\\")\n",
    "    print(\"2. Run: pip install kaggle\")\n",
    "    print(\"3. Check your internet connection\")\n",
    "    raise\n",
    "\n",
    "# Find and load the CSV file\n",
    "csv_files = [f for f in os.listdir(download_dir) if f.endswith('.csv')]\n",
    "if not csv_files:\n",
    "    raise FileNotFoundError(f\"No CSV file found in {download_dir}. Please check the download.\")\n",
    "\n",
    "kaggle_path = os.path.join(download_dir, csv_files[0])\n",
    "df_kaggle = pd.read_csv(kaggle_path)\n",
    "\n",
    "print(f\"\\nâœ“ Kaggle dataset loaded: {len(df_kaggle)} samples\")\n",
    "print(f\"  Features: {list(df_kaggle.columns)}\")\n",
    "\n",
    "# ========================================================================\n",
    "# PART C: Process Kaggle Dataset\n",
    "# ========================================================================\n",
    "print(\"\\n[PART C] Processing Kaggle Dataset...\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Standardize risk categories - convert to string first to avoid categorical issues\n",
    "if 'Landslide_Risk' in df_kaggle.columns:\n",
    "    df_kaggle['Landslide_Risk'] = df_kaggle['Landslide_Risk'].astype(str)\n",
    "    risk_mapping = {'Low': 'Low', 'Moderate': 'Medium', 'High': 'High'}\n",
    "    df_kaggle['rockfall_risk'] = df_kaggle['Landslide_Risk'].map(risk_mapping)\n",
    "    \n",
    "    # Add some Critical samples (20% of High becomes Critical)\n",
    "    high_indices = df_kaggle[df_kaggle['rockfall_risk'] == 'High'].index\n",
    "    if len(high_indices) > 0:\n",
    "        critical_indices = np.random.choice(high_indices, size=int(len(high_indices) * 0.2), replace=False)\n",
    "        df_kaggle.loc[critical_indices, 'rockfall_risk'] = 'Critical'\n",
    "    \n",
    "    df_kaggle = df_kaggle.drop('Landslide_Risk', axis=1)\n",
    "\n",
    "# Handle Soil_Type encoding (one-hot encoding if present)\n",
    "if 'Soil_Type' in df_kaggle.columns:\n",
    "    soil_dummies = pd.get_dummies(df_kaggle['Soil_Type'], prefix='soil')\n",
    "    df_kaggle = pd.concat([df_kaggle, soil_dummies], axis=1)\n",
    "    df_kaggle = df_kaggle.drop('Soil_Type', axis=1)\n",
    "\n",
    "df_kaggle['data_source'] = 'Kaggle'\n",
    "\n",
    "print(f\"âœ“ Risk categories standardized: {df_kaggle['rockfall_risk'].value_counts().to_dict()}\")\n",
    "\n",
    "# ========================================================================\n",
    "# PART D: Merge Datasets\n",
    "# ========================================================================\n",
    "print(\"\\n[PART D] Merging Synthetic + Kaggle Datasets...\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Align columns - add missing features as NaN (we'll handle in preprocessing)\n",
    "all_features = set(df_synthetic.columns) | set(df_kaggle.columns)\n",
    "all_features.discard('rockfall_risk')\n",
    "all_features.discard('data_source')\n",
    "\n",
    "for col in all_features:\n",
    "    if col not in df_synthetic.columns:\n",
    "        df_synthetic[col] = np.nan\n",
    "    if col not in df_kaggle.columns:\n",
    "        df_kaggle[col] = np.nan\n",
    "\n",
    "# Combine datasets\n",
    "df = pd.concat([df_synthetic, df_kaggle], ignore_index=True)\n",
    "\n",
    "print(f\"âœ“ Combined dataset created: {len(df)} total samples\")\n",
    "print(f\"  - Synthetic samples: {len(df_synthetic)}\")\n",
    "print(f\"  - Kaggle samples: {len(df_kaggle)}\")\n",
    "print(f\"\\nâœ“ Total features: {len([c for c in df.columns if c not in ['rockfall_risk', 'data_source']])}\")\n",
    "\n",
    "# ========================================================================\n",
    "# PART E: Data Overview\n",
    "# ========================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMBINED DATASET OVERVIEW\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nDataset Information:\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\nDescriptive Statistics:\")\n",
    "print(df.describe())\n",
    "\n",
    "print(\"\\nClass Distribution:\")\n",
    "print(df['rockfall_risk'].value_counts().sort_index())\n",
    "\n",
    "print(\"\\nData Source Distribution:\")\n",
    "print(df['data_source'].value_counts())\n",
    "\n",
    "# Visualization: Class Distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.countplot(x='rockfall_risk', data=df, order=['Low', 'Medium', 'High', 'Critical'], palette='viridis')\n",
    "plt.title('Combined Dataset - Risk Distribution', fontweight='bold')\n",
    "plt.xlabel('Risk Category')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "risk_by_source = df.groupby(['data_source', 'rockfall_risk']).size().unstack(fill_value=0)\n",
    "risk_by_source.plot(kind='bar', stacked=True, ax=plt.gca(), color=['#2ecc71', '#f39c12', '#e74c3c', '#c0392b'])\n",
    "plt.title('Risk Distribution by Data Source', fontweight='bold')\n",
    "plt.xlabel('Data Source')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='Risk Level', bbox_to_anchor=(1.05, 1))\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the combined dataset\n",
    "output_path = '../data/rockfall_data.csv'\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"\\nâœ“ Combined dataset saved to {output_path}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e0acb5",
   "metadata": {},
   "source": [
    "# Enhanced Exploratory Data Analysis (EDA)\n",
    "\n",
    "Now let's perform deeper analysis including correlation analysis, distribution analysis, and outlier detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881dd55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Correlation Analysis - Heatmap\n",
    "print(\"=== Correlation Analysis ===\\n\")\n",
    "\n",
    "# Get numeric features only (exclude target and source)\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "features = [col for col in numeric_cols if col not in ['rockfall_risk', 'risk_encoded']]\n",
    "\n",
    "print(f\"Analyzing {len(features)} numeric features:\")\n",
    "print(features)\n",
    "\n",
    "# Calculate correlation matrix for features only\n",
    "correlation_matrix = df[features].corr()\n",
    "\n",
    "# Plot correlation heatmap\n",
    "fig_size = max(10, len(features) * 0.8)\n",
    "plt.figure(figsize=(fig_size, fig_size))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8}, annot_kws={'size': 8})\n",
    "plt.title('Correlation Heatmap of All Features (Synthetic + Kaggle)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Strong positive correlations indicate features that tend to increase together\")\n",
    "print(\"- This helps identify redundant features or multicollinearity issues\")\n",
    "print(\"- Note: NaN values from missing features are excluded from correlation calculation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c5faa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Correlation with Target Variable\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode risk levels for correlation analysis\n",
    "le = LabelEncoder()\n",
    "df['risk_encoded'] = le.fit_transform(df['rockfall_risk'])\n",
    "\n",
    "# Calculate correlation with encoded target\n",
    "feature_target_corr = df[features + ['risk_encoded']].corr()['risk_encoded'].drop('risk_encoded').sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "feature_target_corr.plot(kind='barh', color='steelblue')\n",
    "plt.title('Correlation of Features with Rockfall Risk', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Correlation Coefficient')\n",
    "plt.ylabel('Sensor Features')\n",
    "plt.axvline(x=0, color='black', linestyle='--', linewidth=0.8)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFeature-Target Correlations:\")\n",
    "print(feature_target_corr)\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Features with higher correlation have stronger linear relationships with risk level\")\n",
    "print(\"- These are likely to be important predictors in our models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2244a8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Distribution Analysis - Histograms and KDE Plots\n",
    "print(\"\\n=== Distribution Analysis ===\\n\")\n",
    "\n",
    "# Plot for features that have data\n",
    "n_features = len(features)\n",
    "n_cols = 3\n",
    "n_rows = (n_features + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4 * n_rows))\n",
    "axes = axes.ravel() if n_features > 1 else [axes]\n",
    "\n",
    "for idx, feature in enumerate(features):\n",
    "    # Only plot if feature has non-null values\n",
    "    feature_data = df[feature].dropna()\n",
    "    if len(feature_data) > 0:\n",
    "        axes[idx].hist(feature_data, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        feature_data.plot(kind='kde', ax=axes[idx], secondary_y=True, color='red', linewidth=2)\n",
    "        axes[idx].set_title(f'Distribution of {feature}', fontweight='bold', fontsize=10)\n",
    "        axes[idx].set_xlabel(feature, fontsize=9)\n",
    "        axes[idx].set_ylabel('Frequency', fontsize=9)\n",
    "        axes[idx].grid(alpha=0.3)\n",
    "\n",
    "# Remove extra subplots\n",
    "for idx in range(n_features, len(axes)):\n",
    "    fig.delaxes(axes[idx])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Interpretation:\")\n",
    "print(\"- Histograms show the frequency distribution of each feature\")\n",
    "print(\"- KDE (red line) shows the probability density\")\n",
    "print(\"- Some features may show bimodal distributions (two peaks) due to data coming from two sources\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaa24a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Box Plots by Risk Category\n",
    "print(\"\\n=== Distribution by Risk Category ===\\n\")\n",
    "\n",
    "n_features = len(features)\n",
    "n_cols = 3\n",
    "n_rows = (n_features + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 4 * n_rows))\n",
    "axes = axes.ravel() if n_features > 1 else [axes]\n",
    "\n",
    "risk_order = ['Low', 'Medium', 'High', 'Critical']\n",
    "\n",
    "for idx, feature in enumerate(features):\n",
    "    # Only plot if feature has sufficient non-null values\n",
    "    feature_data = df[[feature, 'rockfall_risk']].dropna()\n",
    "    if len(feature_data) > 0:\n",
    "        sns.boxplot(data=feature_data, x='rockfall_risk', y=feature, order=risk_order, \n",
    "                    palette='Set2', ax=axes[idx])\n",
    "        axes[idx].set_title(f'{feature} by Risk Category', fontweight='bold', fontsize=10)\n",
    "        axes[idx].set_xlabel('Risk Level', fontsize=9)\n",
    "        axes[idx].set_ylabel(feature, fontsize=9)\n",
    "        axes[idx].grid(alpha=0.3, axis='y')\n",
    "        axes[idx].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Remove extra subplots\n",
    "for idx in range(n_features, len(axes)):\n",
    "    fig.delaxes(axes[idx])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Interpretation:\")\n",
    "print(\"- Box plots show median (line), quartiles (box), and outliers (dots) for each risk category\")\n",
    "print(\"- Clear separation between risk categories indicates the feature is a good predictor\")\n",
    "print(\"- Note: Some features only have data from one source (synthetic or Kaggle)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b0e049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Pair Plot - Visualizing Feature Relationships\n",
    "print(\"\\n=== Pair Plot Analysis ===\\n\")\n",
    "print(\"Generating pair plot for key features (this may take a moment)...\")\n",
    "\n",
    "# Select top features with most complete data for pair plot (to avoid clutter)\n",
    "feature_completeness = df[features].notna().sum().sort_values(ascending=False)\n",
    "top_features = feature_completeness.head(5).index.tolist()\n",
    "\n",
    "print(f\"Plotting top 5 features with most complete data: {top_features}\")\n",
    "\n",
    "# Sample data for faster plotting\n",
    "sample_size = min(1000, len(df))\n",
    "df_sample = df[top_features + ['rockfall_risk']].dropna().sample(n=min(sample_size, len(df.dropna())), random_state=42)\n",
    "\n",
    "if len(df_sample) > 50:  # Only plot if we have enough data\n",
    "    pairplot = sns.pairplot(df_sample, \n",
    "                            hue='rockfall_risk', \n",
    "                            palette='Set1',\n",
    "                            hue_order=risk_order,\n",
    "                            diag_kind='kde',\n",
    "                            plot_kws={'alpha': 0.6, 's': 30},\n",
    "                            height=2.5)\n",
    "    pairplot.fig.suptitle('Pair Plot: Top Features Colored by Risk Level', \n",
    "                          y=1.01, fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nInterpretation:\")\n",
    "    print(\"- Diagonal: Distribution of each feature by risk category\")\n",
    "    print(\"- Off-diagonal: Scatter plots showing relationships between feature pairs\")\n",
    "    print(\"- Good class separation indicates features work well together for prediction\")\n",
    "else:\n",
    "    print(\"âš  Not enough complete data for pair plot after removing NaN values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1948fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Outlier Detection using IQR Method\n",
    "print(\"\\n=== Outlier Detection ===\\n\")\n",
    "\n",
    "outlier_summary = {}\n",
    "\n",
    "for feature in features:\n",
    "    Q1 = df[feature].quantile(0.25)\n",
    "    Q3 = df[feature].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = df[(df[feature] < lower_bound) | (df[feature] > upper_bound)]\n",
    "    outlier_count = len(outliers)\n",
    "    outlier_percentage = (outlier_count / len(df)) * 100\n",
    "    \n",
    "    outlier_summary[feature] = {\n",
    "        'Count': outlier_count,\n",
    "        'Percentage': f'{outlier_percentage:.2f}%',\n",
    "        'Lower_Bound': f'{lower_bound:.2f}',\n",
    "        'Upper_Bound': f'{upper_bound:.2f}'\n",
    "    }\n",
    "\n",
    "outlier_df = pd.DataFrame(outlier_summary).T\n",
    "print(\"Outlier Summary (using IQR method):\")\n",
    "print(outlier_df)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Decision on Outliers:\")\n",
    "print(\"=\"*60)\n",
    "print(\"Since our data is synthetically generated with intentional high-risk scenarios,\")\n",
    "print(\"these 'outliers' represent critical conditions (high seismic activity, displacement, etc.)\")\n",
    "print(\"that are ESSENTIAL for predicting high/critical risk categories.\")\n",
    "print(\"\\nAction: We will RETAIN all outliers as they contain important information\")\n",
    "print(\"about extreme conditions that lead to rockfall events.\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f3dbf1",
   "metadata": {},
   "source": [
    "# Handling Missing Values from Data Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c73060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Missing Value Analysis\n",
    "print(\"\\n=== Missing Value Analysis ===\\n\")\n",
    "\n",
    "# Check missing values\n",
    "missing_counts = df.isnull().sum()\n",
    "missing_percentages = (missing_counts / len(df)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Feature': missing_counts.index,\n",
    "    'Missing_Count': missing_counts.values,\n",
    "    'Missing_Percentage': missing_percentages.values\n",
    "}).sort_values('Missing_Count', ascending=False)\n",
    "\n",
    "print(\"Missing Values Summary:\")\n",
    "print(missing_df[missing_df['Missing_Count'] > 0])\n",
    "\n",
    "# Visualize missing data pattern\n",
    "plt.figure(figsize=(12, 6))\n",
    "missing_data = missing_df[missing_df['Missing_Count'] > 0]\n",
    "\n",
    "if len(missing_data) > 0:\n",
    "    plt.barh(missing_data['Feature'], missing_data['Missing_Percentage'], color='coral')\n",
    "    plt.xlabel('Missing Percentage (%)', fontweight='bold')\n",
    "    plt.ylabel('Feature', fontweight='bold')\n",
    "    plt.title('Missing Data by Feature', fontweight='bold', fontsize=14)\n",
    "    plt.axvline(x=50, color='red', linestyle='--', label='50% threshold')\n",
    "    plt.legend()\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"âœ“ No missing values detected!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STRATEGY FOR MISSING VALUES:\")\n",
    "print(\"=\"*70)\n",
    "print(\"Why we have missing values:\")\n",
    "print(\"  - Synthetic data has: seismic, vibration, water pressure, displacement, rainfall\")\n",
    "print(\"  - Kaggle data has: temperature, humidity, rain, moisture, slope angle, soil type\")\n",
    "print(\"  - Each source lacks features from the other\")\n",
    "\n",
    "print(\"\\nHandling Strategy (in preprocessing notebook):\")\n",
    "print(\"  Option 1: Use only complete cases (rows with no NaN)\")\n",
    "print(\"  Option 2: Impute missing values with mean/median/mode\")\n",
    "print(\"  Option 3: Train separate models for each data source\")\n",
    "print(\"  Option 4: Use models that handle missing data (e.g., XGBoost)\")\n",
    "\n",
    "print(\"\\nFor this project, we'll use OPTION 1 or 2 in the preprocessing phase.\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rockfall-venv)",
   "language": "python",
   "name": "rockfall-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
