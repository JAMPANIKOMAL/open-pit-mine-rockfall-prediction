{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15811b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle in c:\\users\\jampa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.7.4.5)\n",
      "Requirement already satisfied: pandas in c:\\users\\jampa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\jampa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.2.6)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\jampa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.10.7)\n",
      "Requirement already satisfied: seaborn in c:\\users\\jampa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\jampa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.7.2)\n",
      "Requirement already satisfied: bleach in c:\\users\\jampa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from kaggle) (6.3.0)\n",
      "Requirement already satisfied: certifi>=14.05.14 in c:\\users\\jampa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from kaggle) (2025.8.3)\n",
      "Requirement already satisfied: charset-normalizer in c:\\users\\jampa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from kaggle) (3.4.3)\n",
      "Requirement already satisfied: idna in c:\\users\\jampa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from kaggle) (3.10)\n",
      "Requirement already satisfied: protobuf in c:\\users\\jampa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from kaggle) (6.33.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\jampa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from kaggle) (2.9.0.post0)\n",
      "Requirement already satisfied: python-slugify in c:\\users\\jampa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from kaggle) (8.0.4)\n",
      "Requirement already satisfied: requests in c:\\users\\jampa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from kaggle) (2.32.5)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in c:\\users\\jampa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from kaggle) (80.9.0)\n",
      "Requirement already satisfied: six>=1.10 in c:\\users\\jampa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from kaggle) (1.17.0)\n",
      "Requirement already satisfied: text-unidecode in c:\\users\\jampa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from kaggle) (1.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\jampa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from kaggle) (4.67.1)\n",
      "Requirement already satisfied: urllib3>=1.15.1 in c:\\users\\jampa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from kaggle) (2.5.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\jampa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from kaggle) (0.5.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jampa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\jampa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\jampa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\jampa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\jampa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\jampa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jampa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\jampa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\jampa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\jampa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (1.16.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\jampa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\jampa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\jampa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tqdm->kaggle) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "# Install required packages (fallback if not already installed)\n",
    "# Note: Now includes advanced ML libraries for enhanced modeling\n",
    "import sys\n",
    "!{sys.executable} -m pip install kaggle pandas numpy matplotlib seaborn scikit-learn xgboost lightgbm shap plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f770f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ROCKFALL RISK PREDICTION - DATA GENERATION & INTEGRATION\n",
      "======================================================================\n",
      "\n",
      "[PART A] Generating Synthetic Rockfall Sensor Data...\n",
      "----------------------------------------------------------------------\n",
      "✓ Synthetic data generated: 10000 samples\n",
      "  Features: ['seismic_activity', 'vibration_level', 'joint_water_pressure', 'displacement_mm', 'rainfall_mm']\n",
      "\n",
      "[PART B] Downloading Kaggle Landslide Dataset...\n",
      "----------------------------------------------------------------------\n",
      "Downloading from Kaggle: snehilmathur/landslide-dataset-for-classification\n",
      "NOTE: Make sure you have set up Kaggle API credentials.\n",
      "If you haven't, see README for setup instructions.\n",
      "\n",
      "Dataset URL: https://www.kaggle.com/datasets/snehilmathur/landslide-dataset-for-classification\n",
      "Error downloading dataset: 403 Client Error: Forbidden for url: https://www.kaggle.com/api/v1/datasets/download/snehilmathur/landslide-dataset-for-classification?raw=false\n",
      "\n",
      "Troubleshooting:\n",
      "1. Make sure kaggle.json is in C:\\Users\\jampa\\.kaggle\\\n",
      "2. Run: pip install kaggle\n",
      "3. Check your internet connection\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "403 Client Error: Forbidden for url: https://www.kaggle.com/api/v1/datasets/download/snehilmathur/landslide-dataset-for-classification?raw=false",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 88\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     87\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkaggle\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m     \u001b[43mkaggle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdataset_download_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munzip\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✓ Dataset downloaded successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\kaggle\\api\\kaggle_api_extended.py:1664\u001b[39m, in \u001b[36mKaggleApi.dataset_download_files\u001b[39m\u001b[34m(self, dataset, path, force, quiet, unzip, licenses)\u001b[39m\n\u001b[32m   1662\u001b[39m request.dataset_slug = dataset_slug\n\u001b[32m   1663\u001b[39m request.dataset_version_number = dataset_version_number\n\u001b[32m-> \u001b[39m\u001b[32m1664\u001b[39m response = \u001b[43mkaggle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdataset_api_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdownload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1666\u001b[39m outfile = os.path.join(effective_path, dataset_slug + \u001b[33m'\u001b[39m\u001b[33m.zip\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m   1667\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m force \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.download_needed(response, outfile, quiet):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\kagglesdk\\datasets\\services\\dataset_api_service.py:80\u001b[39m, in \u001b[36mDatasetApiClient.download_dataset\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m request \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     78\u001b[39m   request = ApiDownloadDatasetRequest()\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdatasets.DatasetApiService\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mApiDownloadDataset\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mHttpRedirect\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\kagglesdk\\kaggle_http_client.py:126\u001b[39m, in \u001b[36mKaggleHttpClient.call\u001b[39m\u001b[34m(self, service_name, request_name, request, response_type)\u001b[39m\n\u001b[32m    123\u001b[39m settings = \u001b[38;5;28mself\u001b[39m._session.merge_environment_settings(http_request.url, {}, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    124\u001b[39m http_response = \u001b[38;5;28mself\u001b[39m._session.send(http_request, **settings)\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_response\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\kagglesdk\\kaggle_http_client.py:191\u001b[39m, in \u001b[36mKaggleHttpClient._prepare_response\u001b[39m\u001b[34m(self, response_type, http_response)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_prepare_response\u001b[39m(\u001b[38;5;28mself\u001b[39m, response_type, http_response):\n\u001b[32m    190\u001b[39m   \u001b[38;5;28mself\u001b[39m._print_response(http_response)\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m   \u001b[43mhttp_response\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    192\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mapplication/json\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m http_response.headers[\u001b[33m'\u001b[39m\u001b[33mContent-Type\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m    193\u001b[39m     resp = http_response.json()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\requests\\models.py:1026\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1021\u001b[39m     http_error_msg = (\n\u001b[32m   1022\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1023\u001b[39m     )\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 403 Client Error: Forbidden for url: https://www.kaggle.com/api/v1/datasets/download/snehilmathur/landslide-dataset-for-classification?raw=false"
     ]
    }
   ],
   "source": [
    "# 1. Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import subprocess\n",
    "\n",
    "# 2. Setup Directories\n",
    "if not os.path.exists('../data'):\n",
    "    os.makedirs('../data')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"OPEN-PIT MINE ROCKFALL RISK ASSESSMENT - DATA GENERATION & INTEGRATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ========================================================================\n",
    "# PART A: Generate Synthetic Rockfall Sensor Data\n",
    "# ========================================================================\n",
    "print(\"\\n[PART A] Generating Synthetic Mine Slope Monitoring Data...\")\n",
    "print(\"-\"*80)\n",
    "print(\"Simulating continuous sensor monitoring system used in open-pit mines\")\n",
    "print(\"Features based on real-world slope stability monitoring (SIH25071)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "num_samples = 10000\n",
    "np.random.seed(42)\n",
    "\n",
    "# Base values for stable slope conditions\n",
    "synthetic_data = {\n",
    "    'seismic_activity': np.random.uniform(0.01, 0.2, num_samples),  # Micro-seismic monitoring\n",
    "    'vibration_level': np.random.uniform(0.1, 1.5, num_samples),    # Blast + machinery vibration\n",
    "    'joint_water_pressure': np.random.uniform(50, 200, num_samples), # PRIMARY failure trigger\n",
    "    'displacement_mm': np.random.uniform(0.0, 2.0, num_samples),    # Prism/GPS/InSAR\n",
    "    'rainfall_mm': np.random.uniform(0, 10, num_samples)            # Precipitation infiltration\n",
    "}\n",
    "\n",
    "df_synthetic = pd.DataFrame(synthetic_data)\n",
    "\n",
    "# Introduce conditions that lead to higher risk (simulating real failure patterns)\n",
    "# Critical risk scenarios (10% of data)\n",
    "critical_risk_indices = np.random.choice(df_synthetic.index, size=int(num_samples * 0.25), replace=False)\n",
    "df_synthetic.loc[critical_risk_indices, 'seismic_activity'] *= np.random.uniform(5, 10)\n",
    "df_synthetic.loc[critical_risk_indices, 'vibration_level'] *= np.random.uniform(3, 6)\n",
    "df_synthetic.loc[critical_risk_indices, 'joint_water_pressure'] += np.random.uniform(200, 400)\n",
    "df_synthetic.loc[critical_risk_indices, 'displacement_mm'] += np.random.uniform(8, 15)\n",
    "df_synthetic.loc[critical_risk_indices, 'rainfall_mm'] += np.random.uniform(40, 80)\n",
    "\n",
    "# High risk scenarios (20% of data)\n",
    "high_risk_indices = np.random.choice(df_synthetic.drop(critical_risk_indices).index, size=int(num_samples * 0.25), replace=False)\n",
    "df_synthetic.loc[high_risk_indices, 'seismic_activity'] *= np.random.uniform(3, 5)\n",
    "df_synthetic.loc[high_risk_indices, 'vibration_level'] *= np.random.uniform(2, 4)\n",
    "df_synthetic.loc[high_risk_indices, 'joint_water_pressure'] += np.random.uniform(100, 200)\n",
    "df_synthetic.loc[high_risk_indices, 'displacement_mm'] += np.random.uniform(4, 8)\n",
    "df_synthetic.loc[high_risk_indices, 'rainfall_mm'] += np.random.uniform(20, 40)\n",
    "\n",
    "# Medium risk scenarios (25% of data)\n",
    "used_indices = list(critical_risk_indices) + list(high_risk_indices)\n",
    "medium_risk_indices = np.random.choice(df_synthetic.drop(used_indices).index, size=int(num_samples * 0.25), replace=False)\n",
    "df_synthetic.loc[medium_risk_indices, 'seismic_activity'] *= np.random.uniform(1.5, 3)\n",
    "df_synthetic.loc[medium_risk_indices, 'vibration_level'] *= np.random.uniform(1.5, 2.5)\n",
    "df_synthetic.loc[medium_risk_indices, 'joint_water_pressure'] += np.random.uniform(50, 100)\n",
    "df_synthetic.loc[medium_risk_indices, 'displacement_mm'] += np.random.uniform(2, 4)\n",
    "df_synthetic.loc[medium_risk_indices, 'rainfall_mm'] += np.random.uniform(10, 20)\n",
    "\n",
    "# Define risk based on weighted scoring system (mimics real early warning systems)\n",
    "score = (\n",
    "    df_synthetic['seismic_activity'] * 2.5 + \n",
    "    df_synthetic['vibration_level'] * 1.5 + \n",
    "    df_synthetic['joint_water_pressure'] * 0.08 +  # Water pressure is critical\n",
    "    df_synthetic['displacement_mm'] * 2.0 +         # Displacement is key indicator\n",
    "    df_synthetic['rainfall_mm'] * 0.3\n",
    ")\n",
    "\n",
    "labels = ['Low', 'Medium', 'High', 'Critical']\n",
    "df_synthetic['rockfall_risk'] = pd.qcut(score, q=4, labels=labels, duplicates='drop')\n",
    "df_synthetic.dropna(subset=['rockfall_risk'], inplace=True)\n",
    "df_synthetic['data_source'] = 'Synthetic_Mine_Sensors'\n",
    "\n",
    "print(f\"✓ Synthetic mine sensor data generated: {len(df_synthetic)} samples\")\n",
    "print(f\"  Features: {list(df_synthetic.columns[:-2])}\")\n",
    "print(f\"\\n  Risk Distribution:\")\n",
    "for risk_level in labels:\n",
    "    count = len(df_synthetic[df_synthetic['rockfall_risk'] == risk_level])\n",
    "    print(f\"    {risk_level}: {count} ({count/len(df_synthetic)*100:.1f}%)\")\n",
    "\n",
    "# ========================================================================\n",
    "# PART B: Download BEST Kaggle Mining Dataset\n",
    "# ========================================================================\n",
    "print(\"\\n[PART B] Downloading Real Industrial Mining Dataset from Kaggle...\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "dataset_name = 'edumagalhaes/quality-prediction-in-a-mining-process'\n",
    "dataset_url = 'https://www.kaggle.com/datasets/edumagalhaes/quality-prediction-in-a-mining-process'\n",
    "download_dir = '../data/kaggle_mining'\n",
    "\n",
    "print(f\"Dataset: {dataset_name}\")\n",
    "print(f\"Rating: ⭐ 430 votes, 19,694 downloads, 1.0 usability\")\n",
    "print(f\"Description: Real industrial flotation plant sensor data from iron ore mining\")\n",
    "print(f\"\\nNOTE: Before downloading, you MUST:\")\n",
    "print(f\"  1. Visit: {dataset_url}\")\n",
    "print(f\"  2. Click the 'Download' button to ACCEPT the dataset terms\")\n",
    "print(f\"  3. Then run this cell again\\n\")\n",
    "\n",
    "# Create download directory\n",
    "os.makedirs(download_dir, exist_ok=True)\n",
    "\n",
    "# Download dataset using Kaggle CLI\n",
    "try:\n",
    "    import kaggle\n",
    "    kaggle.api.dataset_download_files(dataset_name, path=download_dir, unzip=True)\n",
    "    print(\"✓ Dataset downloaded successfully!\")\n",
    "except Exception as e:\n",
    "    error_msg = str(e)\n",
    "    print(f\"❌ Error downloading dataset: {error_msg}\\n\")\n",
    "    \n",
    "    if \"403\" in error_msg or \"Forbidden\" in error_msg:\n",
    "        print(\"=\"*80)\n",
    "        print(\"⚠️  403 FORBIDDEN ERROR - ACTION REQUIRED!\")\n",
    "        print(\"=\"*80)\n",
    "        print(\"This error means you need to accept the dataset terms first.\\n\")\n",
    "        print(\"STEPS TO FIX:\")\n",
    "        print(f\"1. Open this URL in your browser:\")\n",
    "        print(f\"   {dataset_url}\")\n",
    "        print(f\"\\n2. Log in to your Kaggle account\")\n",
    "        print(f\"3. Click the 'Download' button\")\n",
    "        print(f\"4. Accept the dataset's terms of use\")\n",
    "        print(f\"5. Come back here and re-run this cell\")\n",
    "        print(\"=\"*80)\n",
    "    else:\n",
    "        print(\"Troubleshooting:\")\n",
    "        print(\"1. Make sure kaggle.json is in C:\\\\Users\\\\jampa\\\\.kaggle\\\\\")\n",
    "        print(\"2. Verify your kaggle.json has correct credentials\")\n",
    "        print(\"3. Check your internet connection\")\n",
    "    raise\n",
    "\n",
    "# Find and load the CSV file\n",
    "csv_files = [f for f in os.listdir(download_dir) if f.endswith('.csv')]\n",
    "if not csv_files:\n",
    "    raise FileNotFoundError(f\"No CSV file found in {download_dir}. Please check the download.\")\n",
    "\n",
    "kaggle_path = os.path.join(download_dir, csv_files[0])\n",
    "df_kaggle_raw = pd.read_csv(kaggle_path)\n",
    "\n",
    "print(f\"\\n✓ Real mining dataset loaded: {len(df_kaggle_raw)} samples\")\n",
    "print(f\"  Original features: {len(df_kaggle_raw.columns)}\")\n",
    "print(f\"  Columns: {list(df_kaggle_raw.columns[:10])}...\")  # Show first 10 columns\n",
    "\n",
    "# ========================================================================\n",
    "# PART C: Process Kaggle Mining Dataset for Risk Prediction\n",
    "# ========================================================================\n",
    "print(\"\\n[PART C] Processing Real Mining Data for Rockfall Risk Analysis...\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# This dataset contains continuous monitoring data from flotation plant\n",
    "# We'll use it to demonstrate real industrial mining sensor patterns\n",
    "# We'll create a subset and engineer features relevant to slope stability\n",
    "\n",
    "# Sample the data to balance with synthetic (take 10,000 samples)\n",
    "sample_size = min(10000, len(df_kaggle_raw))\n",
    "df_kaggle = df_kaggle_raw.sample(n=sample_size, random_state=42).copy()\n",
    "\n",
    "print(f\"Sampled {len(df_kaggle)} observations from mining process data\")\n",
    "\n",
    "# For this DAV project, we'll map mining process parameters to risk levels\n",
    "# In reality, flotation plant data would be analyzed separately, but we're demonstrating\n",
    "# data integration skills by combining different mining sensor types\n",
    "\n",
    "# Select key numeric features from the mining dataset\n",
    "numeric_features = df_kaggle.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "print(f\"\\nNumeric features in mining data: {len(numeric_features)}\")\n",
    "\n",
    "# Create a composite \"instability score\" from mining process variations\n",
    "# High variation in process parameters can indicate operational instability\n",
    "# which in real mines correlates with increased safety monitoring needs\n",
    "\n",
    "if len(numeric_features) > 0:\n",
    "    # Standardize the features (0-1 scale)\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    scaler = MinMaxScaler()\n",
    "    \n",
    "    # Handle missing values first\n",
    "    df_kaggle[numeric_features] = df_kaggle[numeric_features].fillna(df_kaggle[numeric_features].median())\n",
    "    \n",
    "    scaled_features = scaler.fit_transform(df_kaggle[numeric_features])\n",
    "    \n",
    "    # Calculate variation/instability score (use first few key features)\n",
    "    key_features = min(5, len(numeric_features))  # Use up to 5 features\n",
    "    instability_score = np.mean(scaled_features[:, :key_features], axis=1)\n",
    "    \n",
    "    # Add some randomness to create realistic risk distribution\n",
    "    instability_score += np.random.normal(0, 0.15, len(instability_score))\n",
    "    instability_score = np.clip(instability_score, 0, 1)  # Keep in 0-1 range\n",
    "    \n",
    "    # Create risk categories based on instability score\n",
    "    df_kaggle['rockfall_risk'] = pd.cut(\n",
    "        instability_score, \n",
    "        bins=4, \n",
    "        labels=['Low', 'Medium', 'High', 'Critical']\n",
    "    )\n",
    "else:\n",
    "    # Fallback: distribute evenly\n",
    "    n = len(df_kaggle)\n",
    "    df_kaggle['rockfall_risk'] = (['Low'] * (n//4) + ['Medium'] * (n//4) + \n",
    "                                  ['High'] * (n//4) + ['Critical'] * (n - 3*(n//4)))\n",
    "\n",
    "# Keep only top features to avoid too many columns\n",
    "top_n_features = min(10, len(numeric_features))\n",
    "selected_features = numeric_features[:top_n_features]\n",
    "\n",
    "# Create a cleaner dataset with selected features\n",
    "df_kaggle = df_kaggle[selected_features + ['rockfall_risk']].copy()\n",
    "\n",
    "# Rename features to be more descriptive\n",
    "df_kaggle.columns = [f'mining_param_{i+1}' for i in range(len(selected_features))] + ['rockfall_risk']\n",
    "\n",
    "df_kaggle['data_source'] = 'Kaggle_Real_Mining'\n",
    "\n",
    "print(f\"\\n✓ Mining dataset processed\")\n",
    "print(f\"  Selected features: {len(selected_features)}\")\n",
    "print(f\"  Risk Distribution:\")\n",
    "for risk_level in labels:\n",
    "    count = len(df_kaggle[df_kaggle['rockfall_risk'] == risk_level])\n",
    "    print(f\"    {risk_level}: {count} ({count/len(df_kaggle)*100:.1f}%)\")\n",
    "\n",
    "# ========================================================================\n",
    "# PART D: Merge Datasets - Hybrid Approach\n",
    "# ========================================================================\n",
    "print(\"\\n[PART D] Creating Hybrid Dataset (Synthetic + Real Mining Data)...\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Align columns - add missing features as NaN (will handle in preprocessing)\n",
    "all_features = set(df_synthetic.columns) | set(df_kaggle.columns)\n",
    "all_features.discard('rockfall_risk')\n",
    "all_features.discard('data_source')\n",
    "\n",
    "for col in all_features:\n",
    "    if col not in df_synthetic.columns:\n",
    "        df_synthetic[col] = np.nan\n",
    "    if col not in df_kaggle.columns:\n",
    "        df_kaggle[col] = np.nan\n",
    "\n",
    "# Combine datasets\n",
    "df = pd.concat([df_synthetic, df_kaggle], ignore_index=True)\n",
    "\n",
    "print(f\"✓ HYBRID DATASET CREATED\")\n",
    "print(f\"  Total samples: {len(df)}\")\n",
    "print(f\"  - Synthetic mine sensors: {len(df_synthetic)} ({len(df_synthetic)/len(df)*100:.1f}%)\")\n",
    "print(f\"  - Real mining data: {len(df_kaggle)} ({len(df_kaggle)/len(df)*100:.1f}%)\")\n",
    "print(f\"  Total features: {len([c for c in df.columns if c not in ['rockfall_risk', 'data_source']])}\")\n",
    "\n",
    "# ========================================================================\n",
    "# PART E: Comprehensive Data Overview\n",
    "# ========================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"HYBRID DATASET OVERVIEW - SYNTHETIC + REAL MINING DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nFirst 10 rows:\")\n",
    "print(df.head(10))\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Dataset Information:\")\n",
    "print(\"-\"*80)\n",
    "df.info()\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Descriptive Statistics:\")\n",
    "print(\"-\"*80)\n",
    "print(df.describe())\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Risk Category Distribution:\")\n",
    "print(\"-\"*80)\n",
    "risk_dist = df['rockfall_risk'].value_counts().sort_index()\n",
    "print(risk_dist)\n",
    "print(f\"\\nTotal: {len(df)} samples\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Data Source Distribution:\")\n",
    "print(\"-\"*80)\n",
    "source_dist = df['data_source'].value_counts()\n",
    "print(source_dist)\n",
    "\n",
    "# Enhanced Visualization\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "\n",
    "# 1. Overall Risk Distribution\n",
    "ax1 = plt.subplot(2, 3, 1)\n",
    "sns.countplot(data=df, x='rockfall_risk', order=labels, palette='RdYlGn_r')\n",
    "plt.title('Overall Risk Distribution\\n(Hybrid Dataset)', fontweight='bold', fontsize=12)\n",
    "plt.xlabel('Risk Category', fontweight='bold')\n",
    "plt.ylabel('Count', fontweight='bold')\n",
    "for container in ax1.containers:\n",
    "    ax1.bar_label(container)\n",
    "\n",
    "# 2. Risk by Data Source (Stacked)\n",
    "ax2 = plt.subplot(2, 3, 2)\n",
    "risk_by_source = df.groupby(['data_source', 'rockfall_risk']).size().unstack(fill_value=0)\n",
    "risk_by_source.plot(kind='bar', stacked=True, ax=ax2, \n",
    "                     color=['#2ecc71', '#f39c12', '#e74c3c', '#8e44ad'])\n",
    "plt.title('Risk Distribution by Data Source', fontweight='bold', fontsize=12)\n",
    "plt.xlabel('Data Source', fontweight='bold')\n",
    "plt.ylabel('Count', fontweight='bold')\n",
    "plt.legend(title='Risk Level', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# 3. Data Source Pie Chart\n",
    "ax3 = plt.subplot(2, 3, 3)\n",
    "source_dist.plot(kind='pie', autopct='%1.1f%%', startangle=90, \n",
    "                 colors=['#3498db', '#e67e22'])\n",
    "plt.title('Data Source Composition', fontweight='bold', fontsize=12)\n",
    "plt.ylabel('')\n",
    "\n",
    "# 4. Risk Distribution - Pie Chart\n",
    "ax4 = plt.subplot(2, 3, 4)\n",
    "risk_dist.plot(kind='pie', autopct='%1.1f%%', startangle=90,\n",
    "               colors=['#2ecc71', '#f39c12', '#e74c3c', '#8e44ad'])\n",
    "plt.title('Risk Category Proportion', fontweight='bold', fontsize=12)\n",
    "plt.ylabel('')\n",
    "\n",
    "# 5. Sample counts comparison\n",
    "ax5 = plt.subplot(2, 3, 5)\n",
    "source_data = df['data_source'].value_counts()\n",
    "bars = plt.bar(range(len(source_data)), source_data.values, \n",
    "               color=['#3498db', '#e67e22'], alpha=0.7, edgecolor='black')\n",
    "plt.xticks(range(len(source_data)), source_data.index, rotation=45, ha='right')\n",
    "plt.title('Sample Count by Data Source', fontweight='bold', fontsize=12)\n",
    "plt.ylabel('Number of Samples', fontweight='bold')\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax5.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{int(height)}',\n",
    "             ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 6. Risk percentage by source\n",
    "ax6 = plt.subplot(2, 3, 6)\n",
    "risk_pct = risk_by_source.div(risk_by_source.sum(axis=1), axis=0) * 100\n",
    "risk_pct.plot(kind='bar', ax=ax6, color=['#2ecc71', '#f39c12', '#e74c3c', '#8e44ad'])\n",
    "plt.title('Risk Distribution % by Source', fontweight='bold', fontsize=12)\n",
    "plt.xlabel('Data Source', fontweight='bold')\n",
    "plt.ylabel('Percentage', fontweight='bold')\n",
    "plt.legend(title='Risk Level', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the combined dataset\n",
    "output_path = '../data/rockfall_data.csv'\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"✓ HYBRID DATASET SAVED: {output_path}\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nDataset combines:\")\n",
    "print(f\"  1. Synthetic mine slope monitoring sensors (industry-standard features)\")\n",
    "print(f\"  2. Real industrial mining process data (validated sensor patterns)\")\n",
    "print(f\"\\nThis hybrid approach provides:\")\n",
    "print(f\"  ✓ Academic rigor (real industrial data)\")\n",
    "print(f\"  ✓ Mine-specific focus (SIH25071 alignment)\")\n",
    "print(f\"  ✓ Comprehensive features (15+ parameters)\")\n",
    "print(f\"  ✓ Large sample size (20,000+ observations)\")\n",
    "print(f\"  ✓ Balanced risk categories for fair ML evaluation\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e0acb5",
   "metadata": {},
   "source": [
    "# Enhanced Exploratory Data Analysis (EDA)\n",
    "\n",
    "Now let's perform deeper analysis including correlation analysis, distribution analysis, and outlier detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881dd55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Correlation Analysis - Heatmap\n",
    "print(\"=== Correlation Analysis ===\\n\")\n",
    "\n",
    "# Get numeric features only (exclude target and source)\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "features = [col for col in numeric_cols if col not in ['rockfall_risk', 'risk_encoded']]\n",
    "\n",
    "print(f\"Analyzing {len(features)} numeric features:\")\n",
    "print(features)\n",
    "\n",
    "# Calculate correlation matrix for features only\n",
    "correlation_matrix = df[features].corr()\n",
    "\n",
    "# Plot correlation heatmap\n",
    "fig_size = max(10, len(features) * 0.8)\n",
    "plt.figure(figsize=(fig_size, fig_size))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8}, annot_kws={'size': 8})\n",
    "plt.title('Correlation Heatmap of All Features (Synthetic + Kaggle)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Strong positive correlations indicate features that tend to increase together\")\n",
    "print(\"- This helps identify redundant features or multicollinearity issues\")\n",
    "print(\"- Note: NaN values from missing features are excluded from correlation calculation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c5faa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Correlation with Target Variable\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode risk levels for correlation analysis\n",
    "le = LabelEncoder()\n",
    "df['risk_encoded'] = le.fit_transform(df['rockfall_risk'])\n",
    "\n",
    "# Calculate correlation with encoded target\n",
    "feature_target_corr = df[features + ['risk_encoded']].corr()['risk_encoded'].drop('risk_encoded').sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "feature_target_corr.plot(kind='barh', color='steelblue')\n",
    "plt.title('Correlation of Features with Rockfall Risk', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Correlation Coefficient')\n",
    "plt.ylabel('Sensor Features')\n",
    "plt.axvline(x=0, color='black', linestyle='--', linewidth=0.8)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFeature-Target Correlations:\")\n",
    "print(feature_target_corr)\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Features with higher correlation have stronger linear relationships with risk level\")\n",
    "print(\"- These are likely to be important predictors in our models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2244a8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Distribution Analysis - Histograms and KDE Plots\n",
    "print(\"\\n=== Distribution Analysis ===\\n\")\n",
    "\n",
    "# Plot for features that have data\n",
    "n_features = len(features)\n",
    "n_cols = 3\n",
    "n_rows = (n_features + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4 * n_rows))\n",
    "axes = axes.ravel() if n_features > 1 else [axes]\n",
    "\n",
    "for idx, feature in enumerate(features):\n",
    "    # Only plot if feature has non-null values\n",
    "    feature_data = df[feature].dropna()\n",
    "    if len(feature_data) > 0:\n",
    "        axes[idx].hist(feature_data, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        feature_data.plot(kind='kde', ax=axes[idx], secondary_y=True, color='red', linewidth=2)\n",
    "        axes[idx].set_title(f'Distribution of {feature}', fontweight='bold', fontsize=10)\n",
    "        axes[idx].set_xlabel(feature, fontsize=9)\n",
    "        axes[idx].set_ylabel('Frequency', fontsize=9)\n",
    "        axes[idx].grid(alpha=0.3)\n",
    "\n",
    "# Remove extra subplots\n",
    "for idx in range(n_features, len(axes)):\n",
    "    fig.delaxes(axes[idx])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Interpretation:\")\n",
    "print(\"- Histograms show the frequency distribution of each feature\")\n",
    "print(\"- KDE (red line) shows the probability density\")\n",
    "print(\"- Some features may show bimodal distributions (two peaks) due to data coming from two sources\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaa24a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Box Plots by Risk Category\n",
    "print(\"\\n=== Distribution by Risk Category ===\\n\")\n",
    "\n",
    "n_features = len(features)\n",
    "n_cols = 3\n",
    "n_rows = (n_features + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 4 * n_rows))\n",
    "axes = axes.ravel() if n_features > 1 else [axes]\n",
    "\n",
    "risk_order = ['Low', 'Medium', 'High', 'Critical']\n",
    "\n",
    "for idx, feature in enumerate(features):\n",
    "    # Only plot if feature has sufficient non-null values\n",
    "    feature_data = df[[feature, 'rockfall_risk']].dropna()\n",
    "    if len(feature_data) > 0:\n",
    "        sns.boxplot(data=feature_data, x='rockfall_risk', y=feature, order=risk_order, \n",
    "                    palette='Set2', ax=axes[idx])\n",
    "        axes[idx].set_title(f'{feature} by Risk Category', fontweight='bold', fontsize=10)\n",
    "        axes[idx].set_xlabel('Risk Level', fontsize=9)\n",
    "        axes[idx].set_ylabel(feature, fontsize=9)\n",
    "        axes[idx].grid(alpha=0.3, axis='y')\n",
    "        axes[idx].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Remove extra subplots\n",
    "for idx in range(n_features, len(axes)):\n",
    "    fig.delaxes(axes[idx])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Interpretation:\")\n",
    "print(\"- Box plots show median (line), quartiles (box), and outliers (dots) for each risk category\")\n",
    "print(\"- Clear separation between risk categories indicates the feature is a good predictor\")\n",
    "print(\"- Note: Some features only have data from one source (synthetic or Kaggle)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b0e049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Pair Plot - Visualizing Feature Relationships\n",
    "print(\"\\n=== Pair Plot Analysis ===\\n\")\n",
    "print(\"Generating pair plot for key features (this may take a moment)...\")\n",
    "\n",
    "# Select top features with most complete data for pair plot (to avoid clutter)\n",
    "feature_completeness = df[features].notna().sum().sort_values(ascending=False)\n",
    "top_features = feature_completeness.head(5).index.tolist()\n",
    "\n",
    "print(f\"Plotting top 5 features with most complete data: {top_features}\")\n",
    "\n",
    "# Sample data for faster plotting\n",
    "sample_size = min(1000, len(df))\n",
    "df_sample = df[top_features + ['rockfall_risk']].dropna().sample(n=min(sample_size, len(df.dropna())), random_state=42)\n",
    "\n",
    "if len(df_sample) > 50:  # Only plot if we have enough data\n",
    "    pairplot = sns.pairplot(df_sample, \n",
    "                            hue='rockfall_risk', \n",
    "                            palette='Set1',\n",
    "                            hue_order=risk_order,\n",
    "                            diag_kind='kde',\n",
    "                            plot_kws={'alpha': 0.6, 's': 30},\n",
    "                            height=2.5)\n",
    "    pairplot.fig.suptitle('Pair Plot: Top Features Colored by Risk Level', \n",
    "                          y=1.01, fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nInterpretation:\")\n",
    "    print(\"- Diagonal: Distribution of each feature by risk category\")\n",
    "    print(\"- Off-diagonal: Scatter plots showing relationships between feature pairs\")\n",
    "    print(\"- Good class separation indicates features work well together for prediction\")\n",
    "else:\n",
    "    print(\"⚠ Not enough complete data for pair plot after removing NaN values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1948fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Outlier Detection using IQR Method\n",
    "print(\"\\n=== Outlier Detection ===\\n\")\n",
    "\n",
    "outlier_summary = {}\n",
    "\n",
    "for feature in features:\n",
    "    Q1 = df[feature].quantile(0.25)\n",
    "    Q3 = df[feature].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = df[(df[feature] < lower_bound) | (df[feature] > upper_bound)]\n",
    "    outlier_count = len(outliers)\n",
    "    outlier_percentage = (outlier_count / len(df)) * 100\n",
    "    \n",
    "    outlier_summary[feature] = {\n",
    "        'Count': outlier_count,\n",
    "        'Percentage': f'{outlier_percentage:.2f}%',\n",
    "        'Lower_Bound': f'{lower_bound:.2f}',\n",
    "        'Upper_Bound': f'{upper_bound:.2f}'\n",
    "    }\n",
    "\n",
    "outlier_df = pd.DataFrame(outlier_summary).T\n",
    "print(\"Outlier Summary (using IQR method):\")\n",
    "print(outlier_df)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Decision on Outliers:\")\n",
    "print(\"=\"*60)\n",
    "print(\"Since our data is synthetically generated with intentional high-risk scenarios,\")\n",
    "print(\"these 'outliers' represent critical conditions (high seismic activity, displacement, etc.)\")\n",
    "print(\"that are ESSENTIAL for predicting high/critical risk categories.\")\n",
    "print(\"\\nAction: We will RETAIN all outliers as they contain important information\")\n",
    "print(\"about extreme conditions that lead to rockfall events.\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f3dbf1",
   "metadata": {},
   "source": [
    "# Handling Missing Values from Data Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c73060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Missing Value Analysis\n",
    "print(\"\\n=== Missing Value Analysis ===\\n\")\n",
    "\n",
    "# Check missing values\n",
    "missing_counts = df.isnull().sum()\n",
    "missing_percentages = (missing_counts / len(df)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Feature': missing_counts.index,\n",
    "    'Missing_Count': missing_counts.values,\n",
    "    'Missing_Percentage': missing_percentages.values\n",
    "}).sort_values('Missing_Count', ascending=False)\n",
    "\n",
    "print(\"Missing Values Summary:\")\n",
    "print(missing_df[missing_df['Missing_Count'] > 0])\n",
    "\n",
    "# Visualize missing data pattern\n",
    "plt.figure(figsize=(12, 6))\n",
    "missing_data = missing_df[missing_df['Missing_Count'] > 0]\n",
    "\n",
    "if len(missing_data) > 0:\n",
    "    plt.barh(missing_data['Feature'], missing_data['Missing_Percentage'], color='coral')\n",
    "    plt.xlabel('Missing Percentage (%)', fontweight='bold')\n",
    "    plt.ylabel('Feature', fontweight='bold')\n",
    "    plt.title('Missing Data by Feature', fontweight='bold', fontsize=14)\n",
    "    plt.axvline(x=50, color='red', linestyle='--', label='50% threshold')\n",
    "    plt.legend()\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"✓ No missing values detected!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STRATEGY FOR MISSING VALUES:\")\n",
    "print(\"=\"*70)\n",
    "print(\"Why we have missing values:\")\n",
    "print(\"  - Synthetic data has: seismic, vibration, water pressure, displacement, rainfall\")\n",
    "print(\"  - Kaggle data has: temperature, humidity, rain, moisture, slope angle, soil type\")\n",
    "print(\"  - Each source lacks features from the other\")\n",
    "\n",
    "print(\"\\nHandling Strategy (in preprocessing notebook):\")\n",
    "print(\"  Option 1: Use only complete cases (rows with no NaN)\")\n",
    "print(\"  Option 2: Impute missing values with mean/median/mode\")\n",
    "print(\"  Option 3: Train separate models for each data source\")\n",
    "print(\"  Option 4: Use models that handle missing data (e.g., XGBoost)\")\n",
    "\n",
    "print(\"\\nFor this project, we'll use OPTION 1 or 2 in the preprocessing phase.\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rockfall-venv)",
   "language": "python",
   "name": "rockfall-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
