{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f770f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ROCKFALL RISK PREDICTION - DATA GENERATION & INTEGRATION\n",
      "======================================================================\n",
      "\n",
      "[PART A] Generating Synthetic Rockfall Sensor Data...\n",
      "----------------------------------------------------------------------\n",
      "✓ Synthetic data generated: 10000 samples\n",
      "  Features: ['seismic_activity', 'vibration_level', 'joint_water_pressure', 'displacement_mm', 'rainfall_mm']\n",
      "\n",
      "[PART B] Loading Kaggle Landslide Dataset...\n",
      "----------------------------------------------------------------------\n",
      "⚠ Could not download from Kaggle: No module named 'opendatasets'\n",
      "⚠ Creating sample Kaggle-like data instead...\n",
      "✓ Sample Kaggle-like data created: 5000 samples\n",
      "  Features: ['Temperature', 'Humidity', 'Rain', 'Moisture', 'Slope_Angle', 'Soil_Type', 'Landslide_Risk']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot setitem on a Categorical with a new category (Critical), set the categories first",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 130\u001b[39m\n\u001b[32m    128\u001b[39m high_indices = df_kaggle[df_kaggle[\u001b[33m'\u001b[39m\u001b[33mrockfall_risk\u001b[39m\u001b[33m'\u001b[39m] == \u001b[33m'\u001b[39m\u001b[33mHigh\u001b[39m\u001b[33m'\u001b[39m].index\n\u001b[32m    129\u001b[39m critical_indices = np.random.choice(high_indices, size=\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(high_indices) * \u001b[32m0.2\u001b[39m), replace=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m \u001b[43mdf_kaggle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcritical_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrockfall_risk\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m = \u001b[33m'\u001b[39m\u001b[33mCritical\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    132\u001b[39m \u001b[38;5;66;03m# Handle Soil_Type encoding (one-hot encoding)\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mSoil_Type\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m df_kaggle.columns:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jampa\\Documents\\rockfall-prediction-system\\.venv\\Lib\\site-packages\\pandas\\core\\indexing.py:912\u001b[39m, in \u001b[36m_LocationIndexer.__setitem__\u001b[39m\u001b[34m(self, key, value)\u001b[39m\n\u001b[32m    909\u001b[39m \u001b[38;5;28mself\u001b[39m._has_valid_setitem_indexer(key)\n\u001b[32m    911\u001b[39m iloc = \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.name == \u001b[33m\"\u001b[39m\u001b[33miloc\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.obj.iloc\n\u001b[32m--> \u001b[39m\u001b[32m912\u001b[39m \u001b[43miloc\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_setitem_with_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jampa\\Documents\\rockfall-prediction-system\\.venv\\Lib\\site-packages\\pandas\\core\\indexing.py:1943\u001b[39m, in \u001b[36m_iLocIndexer._setitem_with_indexer\u001b[39m\u001b[34m(self, indexer, value, name)\u001b[39m\n\u001b[32m   1940\u001b[39m \u001b[38;5;66;03m# align and set the values\u001b[39;00m\n\u001b[32m   1941\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m take_split_path:\n\u001b[32m   1942\u001b[39m     \u001b[38;5;66;03m# We have to operate column-wise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1943\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_setitem_with_indexer_split_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1944\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1945\u001b[39m     \u001b[38;5;28mself\u001b[39m._setitem_single_block(indexer, value, name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jampa\\Documents\\rockfall-prediction-system\\.venv\\Lib\\site-packages\\pandas\\core\\indexing.py:2036\u001b[39m, in \u001b[36m_iLocIndexer._setitem_with_indexer_split_path\u001b[39m\u001b[34m(self, indexer, value, name)\u001b[39m\n\u001b[32m   2033\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2034\u001b[39m     \u001b[38;5;66;03m# scalar value\u001b[39;00m\n\u001b[32m   2035\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m loc \u001b[38;5;129;01min\u001b[39;00m ilocs:\n\u001b[32m-> \u001b[39m\u001b[32m2036\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_setitem_single_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpi\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jampa\\Documents\\rockfall-prediction-system\\.venv\\Lib\\site-packages\\pandas\\core\\indexing.py:2176\u001b[39m, in \u001b[36m_iLocIndexer._setitem_single_column\u001b[39m\u001b[34m(self, loc, value, plane_indexer)\u001b[39m\n\u001b[32m   2166\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype == np.void:\n\u001b[32m   2167\u001b[39m         \u001b[38;5;66;03m# This means we're expanding, with multiple columns, e.g.\u001b[39;00m\n\u001b[32m   2168\u001b[39m         \u001b[38;5;66;03m#     df = pd.DataFrame({'A': [1,2,3], 'B': [4,5,6]})\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2171\u001b[39m         \u001b[38;5;66;03m# Here, we replace those temporary `np.void` columns with\u001b[39;00m\n\u001b[32m   2172\u001b[39m         \u001b[38;5;66;03m# columns of the appropriate dtype, based on `value`.\u001b[39;00m\n\u001b[32m   2173\u001b[39m         \u001b[38;5;28mself\u001b[39m.obj.iloc[:, loc] = construct_1d_array_from_inferred_fill_value(\n\u001b[32m   2174\u001b[39m             value, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.obj)\n\u001b[32m   2175\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m2176\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcolumn_setitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplane_indexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2178\u001b[39m \u001b[38;5;28mself\u001b[39m.obj._clear_item_cache()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jampa\\Documents\\rockfall-prediction-system\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:1356\u001b[39m, in \u001b[36mBlockManager.column_setitem\u001b[39m\u001b[34m(self, loc, idx, value, inplace_only)\u001b[39m\n\u001b[32m   1354\u001b[39m     col_mgr.setitem_inplace(idx, value)\n\u001b[32m   1355\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1356\u001b[39m     new_mgr = \u001b[43mcol_mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43msetitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.iset(loc, new_mgr._block.values, inplace=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   1359\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m needs_to_warn:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jampa\\Documents\\rockfall-prediction-system\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:434\u001b[39m, in \u001b[36mBaseBlockManager.setitem\u001b[39m\u001b[34m(self, indexer, value, warn)\u001b[39m\n\u001b[32m    430\u001b[39m     \u001b[38;5;66;03m# No need to split if we either set all columns or on a single block\u001b[39;00m\n\u001b[32m    431\u001b[39m     \u001b[38;5;66;03m# manager\u001b[39;00m\n\u001b[32m    432\u001b[39m     \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28mself\u001b[39m.copy()\n\u001b[32m--> \u001b[39m\u001b[32m434\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msetitem\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jampa\\Documents\\rockfall-prediction-system\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:363\u001b[39m, in \u001b[36mBaseBlockManager.apply\u001b[39m\u001b[34m(self, f, align_keys, **kwargs)\u001b[39m\n\u001b[32m    361\u001b[39m         applied = b.apply(f, **kwargs)\n\u001b[32m    362\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m         applied = \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    364\u001b[39m     result_blocks = extend_blocks(applied, result_blocks)\n\u001b[32m    366\u001b[39m out = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).from_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m.axes)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jampa\\Documents\\rockfall-prediction-system\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:2129\u001b[39m, in \u001b[36mEABackedBlock.setitem\u001b[39m\u001b[34m(self, indexer, value, using_cow)\u001b[39m\n\u001b[32m   2126\u001b[39m check_setitem_lengths(indexer, value, values)\n\u001b[32m   2128\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2129\u001b[39m     \u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m]\u001b[49m = value\n\u001b[32m   2130\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[32m   2131\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.dtype, IntervalDtype):\n\u001b[32m   2132\u001b[39m         \u001b[38;5;66;03m# see TestSetitemFloatIntervalWithIntIntervalValues\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jampa\\Documents\\rockfall-prediction-system\\.venv\\Lib\\site-packages\\pandas\\core\\arrays\\_mixins.py:261\u001b[39m, in \u001b[36mNDArrayBackedExtensionArray.__setitem__\u001b[39m\u001b[34m(self, key, value)\u001b[39m\n\u001b[32m    259\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__setitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    260\u001b[39m     key = check_array_indexer(\u001b[38;5;28mself\u001b[39m, key)\n\u001b[32m--> \u001b[39m\u001b[32m261\u001b[39m     value = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_setitem_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    262\u001b[39m     \u001b[38;5;28mself\u001b[39m._ndarray[key] = value\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jampa\\Documents\\rockfall-prediction-system\\.venv\\Lib\\site-packages\\pandas\\core\\arrays\\categorical.py:1590\u001b[39m, in \u001b[36mCategorical._validate_setitem_value\u001b[39m\u001b[34m(self, value)\u001b[39m\n\u001b[32m   1588\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._validate_listlike(value)\n\u001b[32m   1589\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1590\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_scalar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jampa\\Documents\\rockfall-prediction-system\\.venv\\Lib\\site-packages\\pandas\\core\\arrays\\categorical.py:1615\u001b[39m, in \u001b[36mCategorical._validate_scalar\u001b[39m\u001b[34m(self, fill_value)\u001b[39m\n\u001b[32m   1613\u001b[39m     fill_value = \u001b[38;5;28mself\u001b[39m._unbox_scalar(fill_value)\n\u001b[32m   1614\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1615\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m   1616\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot setitem on a Categorical with a new \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1617\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mcategory (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfill_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m), set the categories first\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1618\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1619\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m fill_value\n",
      "\u001b[31mTypeError\u001b[39m: Cannot setitem on a Categorical with a new category (Critical), set the categories first"
     ]
    }
   ],
   "source": [
    "# 1. Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import opendatasets as od\n",
    "\n",
    "# 2. Setup Directories\n",
    "if not os.path.exists('../data'):\n",
    "    os.makedirs('../data')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ROCKFALL RISK PREDICTION - DATA GENERATION & INTEGRATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ========================================================================\n",
    "# PART A: Generate Synthetic Rockfall Sensor Data\n",
    "# ========================================================================\n",
    "print(\"\\n[PART A] Generating Synthetic Rockfall Sensor Data...\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "num_samples = 10000  # Increased from 5000 to 10000\n",
    "np.random.seed(42)\n",
    "\n",
    "# Base values for stable conditions\n",
    "synthetic_data = {\n",
    "    'seismic_activity': np.random.uniform(0.01, 0.2, num_samples),\n",
    "    'vibration_level': np.random.uniform(0.1, 1.5, num_samples),\n",
    "    'joint_water_pressure': np.random.uniform(50, 200, num_samples),\n",
    "    'displacement_mm': np.random.uniform(0.0, 2.0, num_samples),\n",
    "    'rainfall_mm': np.random.uniform(0, 10, num_samples)\n",
    "}\n",
    "\n",
    "df_synthetic = pd.DataFrame(synthetic_data)\n",
    "\n",
    "# Introduce conditions that lead to higher risk\n",
    "high_risk_indices = np.random.choice(df_synthetic.index, size=int(num_samples * 0.1), replace=False)\n",
    "df_synthetic.loc[high_risk_indices, 'seismic_activity'] *= np.random.uniform(3, 8)\n",
    "df_synthetic.loc[high_risk_indices, 'vibration_level'] *= np.random.uniform(2, 5)\n",
    "df_synthetic.loc[high_risk_indices, 'joint_water_pressure'] += np.random.uniform(100, 250)\n",
    "df_synthetic.loc[high_risk_indices, 'displacement_mm'] += np.random.uniform(3, 10)\n",
    "df_synthetic.loc[high_risk_indices, 'rainfall_mm'] += np.random.uniform(20, 50)\n",
    "\n",
    "medium_risk_indices = np.random.choice(df_synthetic.drop(high_risk_indices).index, size=int(num_samples * 0.2), replace=False)\n",
    "df_synthetic.loc[medium_risk_indices, 'seismic_activity'] *= np.random.uniform(1.5, 3)\n",
    "df_synthetic.loc[medium_risk_indices, 'vibration_level'] *= np.random.uniform(1.5, 3)\n",
    "df_synthetic.loc[medium_risk_indices, 'joint_water_pressure'] += np.random.uniform(50, 150)\n",
    "df_synthetic.loc[medium_risk_indices, 'displacement_mm'] += np.random.uniform(1, 4)\n",
    "df_synthetic.loc[medium_risk_indices, 'rainfall_mm'] += np.random.uniform(10, 30)\n",
    "\n",
    "# Define risk based on a scoring system\n",
    "score = (\n",
    "    df_synthetic['seismic_activity'] * 2.0 + \n",
    "    df_synthetic['vibration_level'] * 1.5 + \n",
    "    df_synthetic['joint_water_pressure'] * 0.05 + \n",
    "    df_synthetic['displacement_mm'] * 1.0 + \n",
    "    df_synthetic['rainfall_mm'] * 0.2\n",
    ")\n",
    "\n",
    "labels = ['Low', 'Medium', 'High', 'Critical']\n",
    "df_synthetic['rockfall_risk'] = pd.qcut(score, q=4, labels=labels, duplicates='drop')\n",
    "df_synthetic.dropna(subset=['rockfall_risk'], inplace=True)\n",
    "df_synthetic['data_source'] = 'Synthetic'\n",
    "\n",
    "print(f\"✓ Synthetic data generated: {len(df_synthetic)} samples\")\n",
    "print(f\"  Features: {list(df_synthetic.columns[:-2])}\")\n",
    "\n",
    "# ========================================================================\n",
    "# PART B: Download Kaggle Landslide Dataset\n",
    "# ========================================================================\n",
    "print(\"\\n[PART B] Downloading Kaggle Landslide Dataset...\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "dataset_url = 'https://www.kaggle.com/datasets/snehilmathur/landslide-dataset-for-classification'\n",
    "download_dir = '../data/kaggle_landslide'\n",
    "\n",
    "print(f\"Downloading from Kaggle: {dataset_url}\")\n",
    "print(\"NOTE: You will be prompted to enter your Kaggle credentials.\")\n",
    "print(\"If you haven't set up Kaggle API, see README for setup instructions.\\n\")\n",
    "\n",
    "# Download dataset from Kaggle\n",
    "od.download(dataset_url, download_dir)\n",
    "\n",
    "# Find and load the CSV file\n",
    "csv_files = [f for f in os.listdir(download_dir) if f.endswith('.csv')]\n",
    "if not csv_files:\n",
    "    raise FileNotFoundError(f\"No CSV file found in {download_dir}. Please check the download.\")\n",
    "\n",
    "kaggle_path = os.path.join(download_dir, csv_files[0])\n",
    "df_kaggle = pd.read_csv(kaggle_path)\n",
    "\n",
    "print(f\"\\n✓ Kaggle dataset loaded: {len(df_kaggle)} samples\")\n",
    "print(f\"  Features: {list(df_kaggle.columns)}\")\n",
    "\n",
    "# ========================================================================\n",
    "# PART C: Process Kaggle Dataset\n",
    "# ========================================================================\n",
    "print(\"\\n[PART C] Processing Kaggle Dataset...\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Standardize risk categories - convert to string first to avoid categorical issues\n",
    "if 'Landslide_Risk' in df_kaggle.columns:\n",
    "    df_kaggle['Landslide_Risk'] = df_kaggle['Landslide_Risk'].astype(str)\n",
    "    risk_mapping = {'Low': 'Low', 'Moderate': 'Medium', 'High': 'High'}\n",
    "    df_kaggle['rockfall_risk'] = df_kaggle['Landslide_Risk'].map(risk_mapping)\n",
    "    \n",
    "    # Add some Critical samples (20% of High becomes Critical)\n",
    "    high_indices = df_kaggle[df_kaggle['rockfall_risk'] == 'High'].index\n",
    "    if len(high_indices) > 0:\n",
    "        critical_indices = np.random.choice(high_indices, size=int(len(high_indices) * 0.2), replace=False)\n",
    "        df_kaggle.loc[critical_indices, 'rockfall_risk'] = 'Critical'\n",
    "    \n",
    "    df_kaggle = df_kaggle.drop('Landslide_Risk', axis=1)\n",
    "\n",
    "# Handle Soil_Type encoding (one-hot encoding if present)\n",
    "if 'Soil_Type' in df_kaggle.columns:\n",
    "    soil_dummies = pd.get_dummies(df_kaggle['Soil_Type'], prefix='soil')\n",
    "    df_kaggle = pd.concat([df_kaggle, soil_dummies], axis=1)\n",
    "    df_kaggle = df_kaggle.drop('Soil_Type', axis=1)\n",
    "\n",
    "df_kaggle['data_source'] = 'Kaggle'\n",
    "\n",
    "print(f\"✓ Risk categories standardized: {df_kaggle['rockfall_risk'].value_counts().to_dict()}\")\n",
    "\n",
    "# ========================================================================\n",
    "# PART D: Merge Datasets\n",
    "# ========================================================================\n",
    "print(\"\\n[PART D] Merging Synthetic + Kaggle Datasets...\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Align columns - add missing features as NaN (we'll handle in preprocessing)\n",
    "all_features = set(df_synthetic.columns) | set(df_kaggle.columns)\n",
    "all_features.discard('rockfall_risk')\n",
    "all_features.discard('data_source')\n",
    "\n",
    "for col in all_features:\n",
    "    if col not in df_synthetic.columns:\n",
    "        df_synthetic[col] = np.nan\n",
    "    if col not in df_kaggle.columns:\n",
    "        df_kaggle[col] = np.nan\n",
    "\n",
    "# Combine datasets\n",
    "df = pd.concat([df_synthetic, df_kaggle], ignore_index=True)\n",
    "\n",
    "print(f\"✓ Combined dataset created: {len(df)} total samples\")\n",
    "print(f\"  - Synthetic samples: {len(df_synthetic)}\")\n",
    "print(f\"  - Kaggle samples: {len(df_kaggle)}\")\n",
    "print(f\"\\n✓ Total features: {len([c for c in df.columns if c not in ['rockfall_risk', 'data_source']])}\")\n",
    "\n",
    "# ========================================================================\n",
    "# PART E: Data Overview\n",
    "# ========================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMBINED DATASET OVERVIEW\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nDataset Information:\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\nDescriptive Statistics:\")\n",
    "print(df.describe())\n",
    "\n",
    "print(\"\\nClass Distribution:\")\n",
    "print(df['rockfall_risk'].value_counts().sort_index())\n",
    "\n",
    "print(\"\\nData Source Distribution:\")\n",
    "print(df['data_source'].value_counts())\n",
    "\n",
    "# Visualization: Class Distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.countplot(x='rockfall_risk', data=df, order=['Low', 'Medium', 'High', 'Critical'], palette='viridis')\n",
    "plt.title('Combined Dataset - Risk Distribution', fontweight='bold')\n",
    "plt.xlabel('Risk Category')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "risk_by_source = df.groupby(['data_source', 'rockfall_risk']).size().unstack(fill_value=0)\n",
    "risk_by_source.plot(kind='bar', stacked=True, ax=plt.gca(), color=['#2ecc71', '#f39c12', '#e74c3c', '#c0392b'])\n",
    "plt.title('Risk Distribution by Data Source', fontweight='bold')\n",
    "plt.xlabel('Data Source')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='Risk Level', bbox_to_anchor=(1.05, 1))\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the combined dataset\n",
    "output_path = '../data/rockfall_data.csv'\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"\\n✓ Combined dataset saved to {output_path}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e0acb5",
   "metadata": {},
   "source": [
    "# Enhanced Exploratory Data Analysis (EDA)\n",
    "\n",
    "Now let's perform deeper analysis including correlation analysis, distribution analysis, and outlier detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881dd55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Correlation Analysis - Heatmap\n",
    "print(\"=== Correlation Analysis ===\\n\")\n",
    "\n",
    "# Get numeric features only (exclude target and source)\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "features = [col for col in numeric_cols if col not in ['rockfall_risk', 'risk_encoded']]\n",
    "\n",
    "print(f\"Analyzing {len(features)} numeric features:\")\n",
    "print(features)\n",
    "\n",
    "# Calculate correlation matrix for features only\n",
    "correlation_matrix = df[features].corr()\n",
    "\n",
    "# Plot correlation heatmap\n",
    "fig_size = max(10, len(features) * 0.8)\n",
    "plt.figure(figsize=(fig_size, fig_size))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8}, annot_kws={'size': 8})\n",
    "plt.title('Correlation Heatmap of All Features (Synthetic + Kaggle)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Strong positive correlations indicate features that tend to increase together\")\n",
    "print(\"- This helps identify redundant features or multicollinearity issues\")\n",
    "print(\"- Note: NaN values from missing features are excluded from correlation calculation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c5faa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Correlation with Target Variable\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode risk levels for correlation analysis\n",
    "le = LabelEncoder()\n",
    "df['risk_encoded'] = le.fit_transform(df['rockfall_risk'])\n",
    "\n",
    "# Calculate correlation with encoded target\n",
    "feature_target_corr = df[features + ['risk_encoded']].corr()['risk_encoded'].drop('risk_encoded').sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "feature_target_corr.plot(kind='barh', color='steelblue')\n",
    "plt.title('Correlation of Features with Rockfall Risk', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Correlation Coefficient')\n",
    "plt.ylabel('Sensor Features')\n",
    "plt.axvline(x=0, color='black', linestyle='--', linewidth=0.8)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFeature-Target Correlations:\")\n",
    "print(feature_target_corr)\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Features with higher correlation have stronger linear relationships with risk level\")\n",
    "print(\"- These are likely to be important predictors in our models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2244a8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Distribution Analysis - Histograms and KDE Plots\n",
    "print(\"\\n=== Distribution Analysis ===\\n\")\n",
    "\n",
    "# Plot for features that have data\n",
    "n_features = len(features)\n",
    "n_cols = 3\n",
    "n_rows = (n_features + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4 * n_rows))\n",
    "axes = axes.ravel() if n_features > 1 else [axes]\n",
    "\n",
    "for idx, feature in enumerate(features):\n",
    "    # Only plot if feature has non-null values\n",
    "    feature_data = df[feature].dropna()\n",
    "    if len(feature_data) > 0:\n",
    "        axes[idx].hist(feature_data, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        feature_data.plot(kind='kde', ax=axes[idx], secondary_y=True, color='red', linewidth=2)\n",
    "        axes[idx].set_title(f'Distribution of {feature}', fontweight='bold', fontsize=10)\n",
    "        axes[idx].set_xlabel(feature, fontsize=9)\n",
    "        axes[idx].set_ylabel('Frequency', fontsize=9)\n",
    "        axes[idx].grid(alpha=0.3)\n",
    "\n",
    "# Remove extra subplots\n",
    "for idx in range(n_features, len(axes)):\n",
    "    fig.delaxes(axes[idx])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Interpretation:\")\n",
    "print(\"- Histograms show the frequency distribution of each feature\")\n",
    "print(\"- KDE (red line) shows the probability density\")\n",
    "print(\"- Some features may show bimodal distributions (two peaks) due to data coming from two sources\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaa24a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Box Plots by Risk Category\n",
    "print(\"\\n=== Distribution by Risk Category ===\\n\")\n",
    "\n",
    "n_features = len(features)\n",
    "n_cols = 3\n",
    "n_rows = (n_features + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 4 * n_rows))\n",
    "axes = axes.ravel() if n_features > 1 else [axes]\n",
    "\n",
    "risk_order = ['Low', 'Medium', 'High', 'Critical']\n",
    "\n",
    "for idx, feature in enumerate(features):\n",
    "    # Only plot if feature has sufficient non-null values\n",
    "    feature_data = df[[feature, 'rockfall_risk']].dropna()\n",
    "    if len(feature_data) > 0:\n",
    "        sns.boxplot(data=feature_data, x='rockfall_risk', y=feature, order=risk_order, \n",
    "                    palette='Set2', ax=axes[idx])\n",
    "        axes[idx].set_title(f'{feature} by Risk Category', fontweight='bold', fontsize=10)\n",
    "        axes[idx].set_xlabel('Risk Level', fontsize=9)\n",
    "        axes[idx].set_ylabel(feature, fontsize=9)\n",
    "        axes[idx].grid(alpha=0.3, axis='y')\n",
    "        axes[idx].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Remove extra subplots\n",
    "for idx in range(n_features, len(axes)):\n",
    "    fig.delaxes(axes[idx])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Interpretation:\")\n",
    "print(\"- Box plots show median (line), quartiles (box), and outliers (dots) for each risk category\")\n",
    "print(\"- Clear separation between risk categories indicates the feature is a good predictor\")\n",
    "print(\"- Note: Some features only have data from one source (synthetic or Kaggle)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b0e049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Pair Plot - Visualizing Feature Relationships\n",
    "print(\"\\n=== Pair Plot Analysis ===\\n\")\n",
    "print(\"Generating pair plot for key features (this may take a moment)...\")\n",
    "\n",
    "# Select top features with most complete data for pair plot (to avoid clutter)\n",
    "feature_completeness = df[features].notna().sum().sort_values(ascending=False)\n",
    "top_features = feature_completeness.head(5).index.tolist()\n",
    "\n",
    "print(f\"Plotting top 5 features with most complete data: {top_features}\")\n",
    "\n",
    "# Sample data for faster plotting\n",
    "sample_size = min(1000, len(df))\n",
    "df_sample = df[top_features + ['rockfall_risk']].dropna().sample(n=min(sample_size, len(df.dropna())), random_state=42)\n",
    "\n",
    "if len(df_sample) > 50:  # Only plot if we have enough data\n",
    "    pairplot = sns.pairplot(df_sample, \n",
    "                            hue='rockfall_risk', \n",
    "                            palette='Set1',\n",
    "                            hue_order=risk_order,\n",
    "                            diag_kind='kde',\n",
    "                            plot_kws={'alpha': 0.6, 's': 30},\n",
    "                            height=2.5)\n",
    "    pairplot.fig.suptitle('Pair Plot: Top Features Colored by Risk Level', \n",
    "                          y=1.01, fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nInterpretation:\")\n",
    "    print(\"- Diagonal: Distribution of each feature by risk category\")\n",
    "    print(\"- Off-diagonal: Scatter plots showing relationships between feature pairs\")\n",
    "    print(\"- Good class separation indicates features work well together for prediction\")\n",
    "else:\n",
    "    print(\"⚠ Not enough complete data for pair plot after removing NaN values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1948fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Outlier Detection using IQR Method\n",
    "print(\"\\n=== Outlier Detection ===\\n\")\n",
    "\n",
    "outlier_summary = {}\n",
    "\n",
    "for feature in features:\n",
    "    Q1 = df[feature].quantile(0.25)\n",
    "    Q3 = df[feature].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = df[(df[feature] < lower_bound) | (df[feature] > upper_bound)]\n",
    "    outlier_count = len(outliers)\n",
    "    outlier_percentage = (outlier_count / len(df)) * 100\n",
    "    \n",
    "    outlier_summary[feature] = {\n",
    "        'Count': outlier_count,\n",
    "        'Percentage': f'{outlier_percentage:.2f}%',\n",
    "        'Lower_Bound': f'{lower_bound:.2f}',\n",
    "        'Upper_Bound': f'{upper_bound:.2f}'\n",
    "    }\n",
    "\n",
    "outlier_df = pd.DataFrame(outlier_summary).T\n",
    "print(\"Outlier Summary (using IQR method):\")\n",
    "print(outlier_df)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Decision on Outliers:\")\n",
    "print(\"=\"*60)\n",
    "print(\"Since our data is synthetically generated with intentional high-risk scenarios,\")\n",
    "print(\"these 'outliers' represent critical conditions (high seismic activity, displacement, etc.)\")\n",
    "print(\"that are ESSENTIAL for predicting high/critical risk categories.\")\n",
    "print(\"\\nAction: We will RETAIN all outliers as they contain important information\")\n",
    "print(\"about extreme conditions that lead to rockfall events.\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f3dbf1",
   "metadata": {},
   "source": [
    "# Handling Missing Values from Data Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c73060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Missing Value Analysis\n",
    "print(\"\\n=== Missing Value Analysis ===\\n\")\n",
    "\n",
    "# Check missing values\n",
    "missing_counts = df.isnull().sum()\n",
    "missing_percentages = (missing_counts / len(df)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Feature': missing_counts.index,\n",
    "    'Missing_Count': missing_counts.values,\n",
    "    'Missing_Percentage': missing_percentages.values\n",
    "}).sort_values('Missing_Count', ascending=False)\n",
    "\n",
    "print(\"Missing Values Summary:\")\n",
    "print(missing_df[missing_df['Missing_Count'] > 0])\n",
    "\n",
    "# Visualize missing data pattern\n",
    "plt.figure(figsize=(12, 6))\n",
    "missing_data = missing_df[missing_df['Missing_Count'] > 0]\n",
    "\n",
    "if len(missing_data) > 0:\n",
    "    plt.barh(missing_data['Feature'], missing_data['Missing_Percentage'], color='coral')\n",
    "    plt.xlabel('Missing Percentage (%)', fontweight='bold')\n",
    "    plt.ylabel('Feature', fontweight='bold')\n",
    "    plt.title('Missing Data by Feature', fontweight='bold', fontsize=14)\n",
    "    plt.axvline(x=50, color='red', linestyle='--', label='50% threshold')\n",
    "    plt.legend()\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"✓ No missing values detected!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STRATEGY FOR MISSING VALUES:\")\n",
    "print(\"=\"*70)\n",
    "print(\"Why we have missing values:\")\n",
    "print(\"  - Synthetic data has: seismic, vibration, water pressure, displacement, rainfall\")\n",
    "print(\"  - Kaggle data has: temperature, humidity, rain, moisture, slope angle, soil type\")\n",
    "print(\"  - Each source lacks features from the other\")\n",
    "\n",
    "print(\"\\nHandling Strategy (in preprocessing notebook):\")\n",
    "print(\"  Option 1: Use only complete cases (rows with no NaN)\")\n",
    "print(\"  Option 2: Impute missing values with mean/median/mode\")\n",
    "print(\"  Option 3: Train separate models for each data source\")\n",
    "print(\"  Option 4: Use models that handle missing data (e.g., XGBoost)\")\n",
    "\n",
    "print(\"\\nFor this project, we'll use OPTION 1 or 2 in the preprocessing phase.\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
